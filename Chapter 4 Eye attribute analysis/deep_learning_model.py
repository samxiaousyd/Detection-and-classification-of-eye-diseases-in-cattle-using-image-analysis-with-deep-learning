# -*- coding: utf-8 -*-
"""Deep learning model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNl88vMIKESncPWTJYoXiu6MwsV_PWTa
"""

pip install tensorflow pandas numpy matplotlib

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf

# Paths to your dataset on Google Drive
csv_path = '/content/drive/My Drive/Deep learning model/Set11score.csv'
image_folder = '/content/drive/My Drive/Deep learning model/image11'

# Function to load and preprocess images
def load_and_preprocess_image(image_name):
    path = os.path.join(image_folder, image_name + '.JPG')

    if not os.path.isfile(path):
        print(f"File '{image_name}.JPG' does not exist at {path}")
        return None

    image = cv2.imread(path)

    if image is None:
        print(f"Error: Could not read image {image_name} at {path}")
        return None

    image = cv2.resize(image, (224, 224))
    return image

# Load metadata from CSV file
metadata = pd.read_csv(csv_path)

# Normalize the image IDs
metadata['Id'] = metadata['Id'].str.replace('.JPG', '', regex=False)
metadata['Id'] = metadata['Id'].str.replace('.JPEG', '', regex=False)

# Load images based on the IDs in the CSV
images = [load_and_preprocess_image(row['Id']) for idx, row in metadata.iterrows()]

# Filter out any 'None' entries
X = [img for img in images if img is not None]

# Correctly extract target values based on loaded images
valid_image_ids = [row['Id'] for idx, row in metadata.iterrows() if load_and_preprocess_image(row['Id']) is not None]
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Stained'].values

print(f"Number of images loaded: {len(X)}")
print(f"Number of target rows: {len(y)}")

# Convert the feature arrays to NumPy arrays of type float32
X = np.array(X).astype(np.float32)

# Ensure labels are in the correct format
y = y.astype(str)

# Split data into training, validation, and test sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Check shapes of the data
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_val: {X_val.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_val: {y_val.shape}")
print(f"Shape of y_test: {y_test.shape}")

# Verify unique values in labels after splitting
print(f"Unique values in y_train: {np.unique(y_train)}")
print(f"Unique values in y_val: {np.unique(y_val)}")
print(f"Unique values in y_test: {np.unique(y_test)}")

# Encode the labels to binary format
label_encoder = LabelEncoder()
y_train_binary = label_encoder.fit_transform(y_train.ravel())
y_val_binary = label_encoder.transform(y_val.ravel())
y_test_binary = label_encoder.transform(y_test.ravel())

# Check unique values in binary labels
print(f"Unique values in y_train_binary: {np.unique(y_train_binary)}")
print(f"Unique values in y_val_binary: {np.unique(y_val_binary)}")
print(f"Unique values in y_test_binary: {np.unique(y_test_binary)}")

from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.regularizers import l2
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
import matplotlib.pyplot as plt

# Update 'y' to represent the binary attribute of interest (e.g., 'Stained')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Stained'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched
valid_indices = ~pd.isnull(y)
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after assignment: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temporary sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Encode the labels to integers (0 or 1 for binary classification)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for binary classification
custom_model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary output

# Compile the model with specified hyperparameters
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),  # Initial learning rate
    loss='binary_crossentropy',  # For binary classification
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    epochs=100,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test).flatten()
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy and F1 score
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC for binary classification
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

from sklearn.metrics import precision_recall_curve
# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test_encoded, y_test_pred_probs)

# Find the threshold that gives the best F1 score
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"Best Threshold: {best_threshold}")

# Use the best threshold to get the predicted classes
y_test_pred_classes = (y_test_pred_probs > best_threshold).astype(int)

# Calculate F1 score with the adjusted threshold
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Adjusted Test F1 Score: {test_f1_score:.4f}")

# Update 'y' to represent the binary attribute of interest (e.g., 'Tear')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Tear'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched
valid_indices = ~pd.isnull(y)
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after assignment: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temporary sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Encode the labels to integers (0 or 1 for binary classification)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

pip install keras-tuner

from tensorflow.keras import layers, models, optimizers
from keras_tuner import HyperModel
from keras_tuner.tuners import RandomSearch
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping


# Define the ImageDataGenerator for data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Define the EarlyStopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Define the hypermodel for tuning
class CNNHyperModel(HyperModel):
    def build(self, hp):
        model = models.Sequential()

        # Input layer and data augmentation
        model.add(layers.Input(shape=(224, 224, 3)))
        model.add(layers.RandomFlip("horizontal"))
        model.add(layers.RandomRotation(0.2))
        model.add(layers.RandomZoom(0.2))

        # Convolutional layers with dynamic parameters
        for i in range(hp.Int('conv_layers', 1, 3)):  # Number of Conv layers (1 to 3)
            model.add(layers.Conv2D(
                filters=hp.Choice(f'filters_{i}', values=[32, 64, 128]),  # Choose filters
                kernel_size=hp.Choice(f'kernel_size_{i}', values=[3, 5]),  # Choose kernel size
                activation='relu'
            ))
            model.add(layers.MaxPooling2D((2, 2)))

        model.add(layers.Flatten())

        # Dense layers
        for i in range(hp.Int('dense_layers', 1, 3)):  # Number of Dense layers (1 to 3)
            model.add(layers.Dense(
                units=hp.Int(f'units_{i}', min_value=64, max_value=256, step=64),  # Units in dense layer
                activation='relu',
                kernel_regularizer=l2(0.01)
            ))
            model.add(layers.Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))  # Dropout rate

        # Output layer for binary classification
        model.add(layers.Dense(1, activation='sigmoid'))

        # Compile the model
        model.compile(
            optimizer=optimizers.Adam(learning_rate=hp.Choice('learning_rate', values=[0.01, 0.001, 0.0001])),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )

        return model

# Initialize the tuner for hyperparameter tuning
tuner = RandomSearch(
    CNNHyperModel(),
    objective='val_accuracy',
    max_trials=10,  # Number of hyperparameter combinations to try
    executions_per_trial=2,  # Number of times to run each configuration
    directory='hyperparam_tuning',
    project_name='tear_category_tuning'
)

# Display search space summary
tuner.search_space_summary()

# Start hyperparameter tuning using 'tear' category data
tuner.search(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    validation_data=(X_val, y_val_encoded),
    epochs=20,
    callbacks=[early_stopping]
)

# Get the best hyperparameters
best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"Best hyperparameters for 'tear': {best_hp.values}")

# Build and train the best model with 'tear' data
best_model = tuner.hypermodel.build(best_hp)
history = best_model.fit(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    validation_data=(X_val, y_val_encoded),
    epochs=20,
    callbacks=[early_stopping]
)

# Evaluate the best model on the 'tear' test data
y_test_pred_probs = best_model.predict(X_test).flatten()
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy and F1 score
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)

print(f"Test Accuracy for 'Tear': {test_accuracy:.4f}")
print(f"Test F1 Score for 'Tear': {test_f1_score:.4f}")

# Calculate AUC for binary classification
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC for 'Tear': {test_auc:.4f}")

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for binary classification
custom_model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary output

# Compile the model with specified hyperparameters
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),  # Initial learning rate
    loss='binary_crossentropy',  # For binary classification
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    epochs=100,  # Adjust based on performance
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test).flatten()
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy and F1 score
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test_encoded, y_test_pred_probs)

# Find the threshold that gives the best F1 score
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"Best Threshold: {best_threshold}")

# Use the best threshold to get the predicted classes
y_test_pred_classes = (y_test_pred_probs > best_threshold).astype(int)

# Calculate F1 score with the adjusted threshold
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Adjusted Test F1 Score: {test_f1_score:.4f}")


# Calculate AUC for binary classification
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test_encoded, y_test_pred_probs)

# Find the threshold that gives the best F1 score
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"Best Threshold: {best_threshold}")

# Use the best threshold to get the predicted classes
y_test_pred_classes = (y_test_pred_probs > best_threshold).astype(int)

# Calculate F1 score with the adjusted threshold
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Adjusted Test F1 Score: {test_f1_score:.4f}")

# Update 'y' to represent the attribute of interest (e.g., 'Pscore')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'PScore'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model for multi-class classification
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for multi-class classification
custom_model.add(layers.Dense(num_classes, activation='softmax'))  # Softmax for multi-class output

# Compile the model for multi-class classification
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=32),
    epochs=100,  # Adjust as needed
    validation_data=(X_val, y_val_onehot),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Tear volume')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Tear volume'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model for multi-class classification
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for multi-class classification
custom_model.add(layers.Dense(num_classes, activation='softmax'))  # Softmax for multi-class output

# Compile the model for multi-class classification
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=32),
    epochs=100,  # Adjust as needed
    validation_data=(X_val, y_val_onehot),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Cornea opacity visible')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacity visible'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched
valid_indices = ~pd.isnull(y)
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after assignment: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temporary sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Encode the labels to integers (0 or 1 for binary classification)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for binary classification
custom_model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary output

# Compile the model with specified hyperparameters
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),  # Initial learning rate
    loss='binary_crossentropy',  # For binary classification
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    epochs=100,  # Adjust based on performance
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test).flatten()
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy and F1 score
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test_encoded, y_test_pred_probs)

# Find the threshold that gives the best F1 score
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"Best Threshold: {best_threshold}")

# Use the best threshold to get the predicted classes
y_test_pred_classes = (y_test_pred_probs > best_threshold).astype(int)

# Calculate F1 score with the adjusted threshold
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Adjusted Test F1 Score: {test_f1_score:.4f}")


# Calculate AUC for binary classification
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Replace missing values in 'Cornea opacity colour' with 'null' where 'Cornea opacity visible' is 'no'
metadata.loc[metadata['Cornea opacity visible'] == 'no', 'Cornea opacity colour'] = 'null'

# Update 'y' to represent the attribute of interest (e.g., 'Cornea opacity colour')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacity colour'].values
y = y.astype(str)

# Split data into training and temp sets without stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (including 'null' as a separate class)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)

# Filter validation and test sets to keep only classes present in training
valid_classes = set(y_train)

# Filter validation set
valid_indices_val = [i for i, label in enumerate(y_val) if label in valid_classes]
X_val = X_val[valid_indices_val]
y_val = y_val[valid_indices_val]

# Filter test set
valid_indices_test = [i for i, label in enumerate(y_test) if label in valid_classes]
X_test = X_test[valid_indices_test]
y_test = y_test[valid_indices_test]

# Encode the filtered validation and test labels
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model for multi-class classification
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for multi-class classification
custom_model.add(layers.Dense(num_classes, activation='softmax'))  # Softmax for multi-class output

# Compile the model for multi-class classification
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=32),
    epochs=100,  # Adjust as needed
    validation_data=(X_val, y_val_onehot),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Cornea opacitiy touches limbus')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacitiy touches limbus'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model for multi-class classification
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for multi-class classification
custom_model.add(layers.Dense(num_classes, activation='softmax'))  # Softmax for multi-class output

# Compile the model for multi-class classification
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=32),
    epochs=100,  # Adjust as needed
    validation_data=(X_val, y_val_onehot),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Cornea opaqueness')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opaqueness'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model for multi-class classification
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for multi-class classification
custom_model.add(layers.Dense(num_classes, activation='softmax'))  # Softmax for multi-class output

# Compile the model for multi-class classification
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=32),
    epochs=100,  # Adjust as needed
    validation_data=(X_val, y_val_onehot),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Cornea opacity size')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacity size'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model for multi-class classification
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for multi-class classification
custom_model.add(layers.Dense(num_classes, activation='softmax'))  # Softmax for multi-class output

# Compile the model for multi-class classification
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=32),
    epochs=100,  # Adjust as needed
    validation_data=(X_val, y_val_onehot),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Corneal surface')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal surface'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model for multi-class classification
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for multi-class classification
custom_model.add(layers.Dense(num_classes, activation='softmax'))  # Softmax for multi-class output

# Compile the model for multi-class classification
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_onehot, batch_size=32),
    epochs=100,  # Adjust as needed
    validation_data=(X_val, y_val_onehot),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: hedges')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: hedges'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched
valid_indices = ~pd.isnull(y)
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after assignment: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temporary sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Encode the labels to integers (0 or 1 for binary classification)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for binary classification
custom_model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary output

# Compile the model with specified hyperparameters
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),  # Initial learning rate
    loss='binary_crossentropy',  # For binary classification
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    epochs=100,  # Adjust based on performance
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test).flatten()
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy and F1 score
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test_encoded, y_test_pred_probs)

# Find the threshold that gives the best F1 score
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"Best Threshold: {best_threshold}")

# Use the best threshold to get the predicted classes
y_test_pred_classes = (y_test_pred_probs > best_threshold).astype(int)

# Calculate F1 score with the adjusted threshold
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Adjusted Test F1 Score: {test_f1_score:.4f}")


# Calculate AUC for binary classification
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: tree')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: tree'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched
valid_indices = ~pd.isnull(y)
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after assignment: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temporary sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Encode the labels to integers (0 or 1 for binary classification)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for binary classification
custom_model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary output

# Compile the model with specified hyperparameters
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),  # Initial learning rate
    loss='binary_crossentropy',  # For binary classification
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    epochs=100,  # Adjust based on performance
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test).flatten()
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy and F1 score
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test_encoded, y_test_pred_probs)

# Find the threshold that gives the best F1 score
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"Best Threshold: {best_threshold}")

# Use the best threshold to get the predicted classes
y_test_pred_classes = (y_test_pred_probs > best_threshold).astype(int)

# Calculate F1 score with the adjusted threshold
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Adjusted Test F1 Score: {test_f1_score:.4f}")


# Calculate AUC for binary classification
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: across lesion')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: across lesion'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched
valid_indices = ~pd.isnull(y)
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after assignment: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temporary sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Encode the labels to integers (0 or 1 for binary classification)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for binary classification
custom_model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary output

# Compile the model with specified hyperparameters
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),  # Initial learning rate
    loss='binary_crossentropy',  # For binary classification
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    epochs=100,  # Adjust based on performance
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test).flatten()
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy and F1 score
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test_encoded, y_test_pred_probs)

# Find the threshold that gives the best F1 score
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"Best Threshold: {best_threshold}")

# Use the best threshold to get the predicted classes
y_test_pred_classes = (y_test_pred_probs > best_threshold).astype(int)

# Calculate F1 score with the adjusted threshold
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Adjusted Test F1 Score: {test_f1_score:.4f}")


# Calculate AUC for binary classification
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: across lesion')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: across lesion'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched
valid_indices = ~pd.isnull(y)
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after assignment: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temporary sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42
)

# Encode the labels to integers (0 or 1 for binary classification)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Data Augmentation
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2)
])

# Build a custom CNN model
custom_model = models.Sequential()

# Input layer and data augmentation
custom_model.add(layers.Input(shape=(224, 224, 3)))
custom_model.add(data_augmentation)

# Add convolutional and pooling layers
custom_model.add(layers.Conv2D(32, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(64, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

custom_model.add(layers.Conv2D(128, (3, 3), activation='relu'))
custom_model.add(layers.MaxPooling2D((2, 2)))

# Flatten the feature maps
custom_model.add(layers.Flatten())

# Dense layers
custom_model.add(layers.Dense(128, activation='relu'))
custom_model.add(layers.Dropout(0.2))  # Dropout to prevent overfitting
custom_model.add(layers.Dense(64, activation='relu', kernel_regularizer=l2(0.01)))

# Output layer for binary classification
custom_model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary output

# Compile the model with specified hyperparameters
custom_model.compile(
    optimizer=optimizers.Adam(learning_rate=0.01),  # Initial learning rate
    loss='binary_crossentropy',  # For binary classification
    metrics=['accuracy']
)

# Define early stopping callback
early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Apply ImageDataGenerator for dynamic data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True
)

# Fit the model with data augmentation
history = custom_model.fit(
    datagen.flow(X_train, y_train_encoded, batch_size=32),
    epochs=100,  # Adjust based on performance
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the custom CNN model on the test data
y_test_pred_probs = custom_model.predict(X_test).flatten()
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy and F1 score
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate precision-recall curve
precision, recall, thresholds = precision_recall_curve(y_test_encoded, y_test_pred_probs)

# Find the threshold that gives the best F1 score
f1_scores = 2 * (precision * recall) / (precision + recall + 1e-7)  # Avoid division by zero
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"Best Threshold: {best_threshold}")

# Use the best threshold to get the predicted classes
y_test_pred_classes = (y_test_pred_probs > best_threshold).astype(int)

# Calculate F1 score with the adjusted threshold
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Adjusted Test F1 Score: {test_f1_score:.4f}")


# Calculate AUC for binary classification
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")