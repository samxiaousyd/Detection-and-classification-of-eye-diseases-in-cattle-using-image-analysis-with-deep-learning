# -*- coding: utf-8 -*-
"""Chapter 4 Eye Attribute DL Codes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g2IOPiYGJ-Dxf0LsL1o-Jw4BVoFVtiIn
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import cv2
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    roc_auc_score,
    cohen_kappa_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
)

import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import load_model

# Paths (same as in your old notebook)
csv_path = '/content/drive/My Drive/Deep learning model/Cropped.csv'
image_folder = '/content/drive/My Drive/Deep learning model/Cropped'

# Safety checks
if not os.path.isfile(csv_path):
    raise FileNotFoundError(f"CSV file not found at {csv_path}")
if not os.path.isdir(image_folder):
    raise FileNotFoundError(f"Image folder not found at {image_folder}")

metadata = pd.read_csv(csv_path)

# Make sure columns exist
if 'image_id' not in metadata.columns or 'stained' not in metadata.columns:
    raise KeyError("Required columns 'image_id' and 'stained' not found in CSV file.")

def load_and_preprocess_image(image_name):
    """
    image_name should be the base id from 'image_id' without extension issues.
    This follows your original 224x224 + BGR->RGB pipeline via cv2.
    """
    # image_id in Cropped.csv already includes extension?
    # In your old code you stripped .JPG/.JPEG and then re-added .JPG.
    # Here we mirror that logic.
    base_id = str(image_name).replace('.JPG', '').replace('.JPEG', '')
    path_jpg  = os.path.join(image_folder, base_id + '.JPG')
    path_jpeg = os.path.join(image_folder, base_id + '.JPEG')

    if os.path.isfile(path_jpg):
        path = path_jpg
    elif os.path.isfile(path_jpeg):
        path = path_jpeg
    else:
        # If neither exists, skip
        return None

    img = cv2.imread(path)
    if img is None:
        return None

    img = cv2.resize(img, (224, 224))
    img = img.astype(np.float32)
    return img

# Load all images
images = [load_and_preprocess_image(row['image_id']) for _, row in metadata.iterrows()]

# Filter out missing images and keep matching IDs
valid = [(img, row['image_id']) for img, (_, row) in zip(images, metadata.iterrows()) if img is not None]
if not valid:
    raise RuntimeError("No valid images were loaded. Check paths / filenames.")

X, valid_image_ids = zip(*valid)
X = np.array(X, dtype=np.float32)

# Match labels to valid images
y = metadata.loc[metadata['image_id'].isin(valid_image_ids), 'stained'].values.astype(str)

print(f"Number of valid images: {len(X)}")
print(f"Number of labels: {len(y)}")
print("Unique labels:", np.unique(y))

# 70 : 10 : 20 split (train : val : test)

# First split: 70% train, 30% temp
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.30,          # 30% goes into temp
    random_state=42,
    stratify=y
)

# Second split: from that 30%, make 10% val and 20% test
# 10% of total = 1/3 of 30%  → val
# 20% of total = 2/3 of 30%  → test
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=2/3,           # 2/3 of 30% = 20% of total → test
    random_state=42,
    stratify=y_temp
)

print("Shapes:")
print("X_train:", X_train.shape, "X_val:", X_val.shape, "X_test:", X_test.shape)

# Encode 'stained' to 0/1 as before
label_encoder = LabelEncoder()
y_train_binary = label_encoder.fit_transform(y_train)
y_val_binary   = label_encoder.transform(y_val)
y_test_binary  = label_encoder.transform(y_test)

print("Encoded classes:", list(label_encoder.classes_))
print("y_train_binary unique:", np.unique(y_train_binary))
print("y_val_binary unique:", np.unique(y_val_binary))
print("y_test_binary unique:", np.unique(y_test_binary))

# (Optional but good) label distribution check
def print_label_distribution(labels, name):
    u, c = np.unique(labels, return_counts=True)
    print(f"{name} distribution:", dict(zip(u, c)))

print_label_distribution(y_train, "Train")
print_label_distribution(y_val,   "Val")
print_label_distribution(y_test,  "Test")

np.save('/content/drive/MyDrive/Deep learning model/X_full.npy', X)
np.save('/content/drive/MyDrive/Deep learning model/y_full_stained.npy', y)

print("Saved X and y arrays:", X.shape, y.shape)



model_path = '/content/drive/My Drive/Deep learning model/stained_model.h5'
model = load_model(model_path)

model.summary()

# Predicted probabilities for the positive class
y_test_pred_probs = model.predict(X_test).ravel()   # shape (N,)

# Convert to hard labels using 0.5 threshold
y_test_pred_classes = (y_test_pred_probs >= 0.5).astype(int)

# Point metrics (test set)
test_accuracy = accuracy_score(y_test_binary, y_test_pred_classes)
test_f1       = f1_score(y_test_binary, y_test_pred_classes)
test_auc      = roc_auc_score(y_test_binary, y_test_pred_probs)
test_kappa    = cohen_kappa_score(y_test_binary, y_test_pred_classes)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test F1:       {test_f1:.4f}")
print(f"Test AUC:      {test_auc:.4f}")
print(f"Test Kappa:    {test_kappa:.4f}")

print("\nClassification report:")
print(classification_report(y_test_binary, y_test_pred_classes, target_names=label_encoder.classes_))

# Confusion matrix (optional, just to check it matches your old one)
cm = confusion_matrix(y_test_binary, y_test_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
disp.plot(cmap='Blues')
plt.title('Stained – Test Confusion Matrix')
plt.show()

y_test_pred_probs = model.predict(X_test).ravel()
...
y_test_binary
np.save('/content/drive/MyDrive/Deep learning model/stained_y_test_binary.npy', y_test_binary)
np.save('/content/drive/MyDrive/Deep learning model/stained_y_test_pred_probs.npy', y_test_pred_probs)

print("Saved stained test labels and predicted probabilities!")

import numpy as np

def compute_metrics_binary(y_true, y_prob, threshold=0.5):
    y_true = np.asarray(y_true)
    y_prob = np.asarray(y_prob).ravel()
    y_pred = (y_prob >= threshold).astype(int)

    acc   = accuracy_score(y_true, y_pred)
    f1    = f1_score(y_true, y_pred)
    kappa = cohen_kappa_score(y_true, y_pred)

    try:
        auc = roc_auc_score(y_true, y_prob)
    except ValueError:
        auc = np.nan

    return {"Accuracy": acc, "F1": f1, "AUC": auc, "Kappa": kappa}


def bootstrap_ci_binary(y_true, y_prob, B=1000, random_state=42):
    rng = np.random.default_rng(random_state)
    y_true = np.asarray(y_true)
    y_prob = np.asarray(y_prob).ravel()
    n = len(y_true)

    point = compute_metrics_binary(y_true, y_prob)
    metrics_boot = {k: [] for k in point.keys()}

    for b in range(B):
        idx = rng.integers(0, n, size=n)
        yt = y_true[idx]
        yp = y_prob[idx]
        m = compute_metrics_binary(yt, yp)
        for k, v in m.items():
            metrics_boot[k].append(v)

    ci = {}
    for k, vals in metrics_boot.items():
        vals = np.array(vals)
        vals = vals[~np.isnan(vals)]
        ci[k] = (np.quantile(vals, 0.025), np.quantile(vals, 0.975))

    return point, ci

point_est, ci_est = bootstrap_ci_binary(y_test_binary, y_test_pred_probs, B=1000, random_state=42)

print("Point estimates (test):")
for k, v in point_est.items():
    print(f"{k}: {v:.4f}")

print("\n95% CIs (test):")
for k, (lo, hi) in ci_est.items():
    print(f"{k}: {lo:.4f} – {hi:.4f}")

print(X.shape, y.shape)
print(len(y_test_binary), len(y_test_pred_probs))

np.save('/content/drive/My Drive/Deep learning model/stained_y_test_binary.npy', y_test_binary)
np.save('/content/drive/My Drive/Deep learning model/stained_y_test_pred_probs.npy', y_test_pred_probs)

import os
import numpy as np
import pandas as pd
import cv2

# Paths (same as before)
csv_path = '/content/drive/My Drive/Deep learning model/Cropped.csv'
image_folder = '/content/drive/My Drive/Deep learning model/Cropped'

metadata = pd.read_csv(csv_path)

if 'image_id' not in metadata.columns:
    raise KeyError("Column 'image_id' not found in CSV.")

print("Total rows in original metadata:", len(metadata))

IMG_SIZE = 224  # same as before

def load_and_preprocess_image(image_name):
    """
    Load image by trying .JPG and .JPEG and resize to 224x224.
    """
    base_id = str(image_name).replace('.JPG', '').replace('.JPEG', '')
    path_jpg  = os.path.join(image_folder, base_id + '.JPG')
    path_jpeg = os.path.join(image_folder, base_id + '.JPEG')

    if os.path.isfile(path_jpg):
        path = path_jpg
    elif os.path.isfile(path_jpeg):
        path = path_jpeg
    else:
        return None

    img = cv2.imread(path)
    if img is None:
        return None

    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    img = img.astype(np.float32)
    return img

X_list = []
id_list = []

for _, row in metadata.iterrows():
    img = load_and_preprocess_image(row['image_id'])
    if img is not None:
        X_list.append(img)
        id_list.append(row['image_id'])

if len(X_list) == 0:
    raise RuntimeError("No valid images were loaded. Check paths / filenames.")

X_all = np.stack(X_list, axis=0)
valid_image_ids = np.array(id_list)

print("Number of valid images:", X_all.shape[0])
print("X_all shape:", X_all.shape)

# Normalise
X_all = X_all / 255.0

# Align metadata to X_all ordering so row i ↔ image i
metadata_valid = (
    metadata[metadata['image_id'].isin(valid_image_ids)]
    .set_index('image_id')
    .loc[valid_image_ids]
    .reset_index()
)

print("Rows in metadata_valid:", len(metadata_valid))

# Save for reuse
np.save('/content/X_all.npy', X_all)
metadata_valid.to_csv('/content/metadata_valid.csv', index=False)

target_col = 'ExactColumnNameHere'

metadata_valid.columns.tolist()

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    roc_auc_score,
    cohen_kappa_score,
    classification_report
)

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Load shared preprocessed data
X_all = np.load('/content/X_all.npy')
metadata_valid = pd.read_csv('/content/metadata_valid.csv')

print("X_all shape:", X_all.shape)
print("metadata_valid shape:", metadata_valid.shape)
print("Columns:", metadata_valid.columns.tolist())

# ---- TEAR ATTRIBUTE ----
target_col = 'tear'   # <-- lowercase

if target_col not in metadata_valid.columns:
    raise KeyError(f"Column '{target_col}' not found in metadata_valid.")

# Handle 'Unable to determine' as missing, if present
metadata_valid[target_col] = metadata_valid[target_col].replace(
    ['Unable to determine', 'unable to determine', 'Undetermined'],
    np.nan
)

mask = metadata_valid[target_col].notna()
metadata_attr = metadata_valid[mask].reset_index(drop=True)
X_attr = X_all[mask.values]

print("After dropping NAs for tear:")
print("X_attr shape:", X_attr.shape)
print(metadata_attr[target_col].value_counts())

le = LabelEncoder()
y_raw = metadata_attr[target_col].astype(str).values
y_encoded = le.fit_transform(y_raw)

print("Label classes:", le.classes_)
print("Encoded value counts:", np.unique(y_encoded, return_counts=True))

X_train_val, X_test, y_train_val, y_test = train_test_split(
    X_attr,
    y_encoded,
    test_size=0.20,
    stratify=y_encoded,
    random_state=SEED
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train_val,
    y_train_val,
    test_size=0.25,          # 0.25 of 0.8 = 0.20 overall
    stratify=y_train_val,
    random_state=SEED
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape,   y_val.shape)
print("Test: ", X_test.shape,  y_test.shape)

y_train_binary = y_train
y_val_binary   = y_val
y_test_binary  = y_test

import tensorflow as tf
from tensorflow.keras.models import load_model

# Define a "patched" DepthwiseConv2D that ignores the 'groups' argument
class PatchedDepthwiseConv2D(tf.keras.layers.DepthwiseConv2D):
    @classmethod
    def from_config(cls, config):
        # Safely remove 'groups' if it exists in the saved config
        config.pop('groups', None)
        return super().from_config(config)

# Now load the model using this patched layer
model = load_model(
    '/content/drive/My Drive/Deep learning model/tear_model.h5',
    compile=False,
    custom_objects={'DepthwiseConv2D': PatchedDepthwiseConv2D}
)

# Recompile for evaluation
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score, classification_report

y_test_pred_probs = model.predict(X_test).ravel()
y_test_pred_classes = (y_test_pred_probs >= 0.5).astype(int)

test_accuracy = accuracy_score(y_test_binary, y_test_pred_classes)
test_f1       = f1_score(y_test_binary, y_test_pred_classes)
test_auc      = roc_auc_score(y_test_binary, y_test_pred_probs)
test_kappa    = cohen_kappa_score(y_test_binary, y_test_pred_classes)

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test F1:       {test_f1:.4f}")
print(f"Test AUC:      {test_auc:.4f}")
print(f"Test Kappa:    {test_kappa:.4f}")

print("\nClassification Report:")
print(classification_report(y_test_binary, y_test_pred_classes, target_names=['no_tear', 'tear']))

import numpy as np

def bootstrap_binary_metrics(
    y_true,
    y_prob,
    n_bootstrap=1000,
    random_state=42,
    threshold=0.5
):
    rng = np.random.RandomState(random_state)
    n_samples = len(y_true)

    accs, f1s, aucs, kappas = [], [], [], []

    y_true = np.array(y_true)
    y_prob = np.array(y_prob)

    for _ in range(n_bootstrap):
        idx = rng.randint(0, n_samples, n_samples)

        y_true_b = y_true[idx]
        y_prob_b = y_prob[idx]
        y_pred_b = (y_prob_b >= threshold).astype(int)

        accs.append(accuracy_score(y_true_b, y_pred_b))

        try:
            f1s.append(f1_score(y_true_b, y_pred_b))
        except ValueError:
            pass

        try:
            aucs.append(roc_auc_score(y_true_b, y_prob_b))
        except ValueError:
            pass

        try:
            kappas.append(cohen_kappa_score(y_true_b, y_pred_b))
        except ValueError:
            pass

    def ci(arr):
        arr = np.array(arr)
        return np.percentile(arr, 2.5), np.percentile(arr, 97.5)

    return {
        "accuracy": {"point": test_accuracy, "ci": ci(accs)},
        "f1":       {"point": test_f1,      "ci": ci(f1s)},
        "auc":      {"point": test_auc,     "ci": ci(aucs)},
        "kappa":    {"point": test_kappa,   "ci": ci(kappas)}
    }

bootstrap_results_Tear = bootstrap_binary_metrics(
    y_true=y_test_binary,
    y_prob=y_test_pred_probs,
    n_bootstrap=1000,
    random_state=42
)

for metric_name, res in bootstrap_results_Tear.items():
    point  = res["point"]
    lower, upper = res["ci"]
    print(f"{metric_name.capitalize():8s}: {point:.3f} (95% CI {lower:.3f} – {upper:.3f})")

import numpy as np

# Folder path where you want to save (adjust if you like)
base_path = '/content/drive/My Drive/Deep learning model'

np.save(f'{base_path}/tear_X_test.npy', X_test)
np.save(f'{base_path}/tear_y_test.npy', y_test_binary)
np.save(f'{base_path}/tear_y_test_pred_probs.npy', y_test_pred_probs)
np.save(f'{base_path}/tear_y_test_pred_classes.npy', y_test_pred_classes)

print("Saved tear test arrays (X_test, y_test, probs, preds).")

import pandas as pd

# bootstrap_results_Tear should look like:
# {
#   "accuracy": {"point": ..., "ci": (lower, upper)},
#   "f1":       {"point": ..., "ci": (lower, upper)},
#   "auc":      {"point": ..., "ci": (lower, upper)},
#   "kappa":    {"point": ..., "ci": (lower, upper)}
# }

rows = []
for metric_name, res in bootstrap_results_Tear.items():
    point  = res["point"]
    lower, upper = res["ci"]
    rows.append({
        "metric": metric_name,
        "point": point,
        "ci_lower": lower,
        "ci_upper": upper,
        "formatted": f"{point:.3f} ({lower:.3f}–{upper:.3f})"
    })

df_boot_tear = pd.DataFrame(rows)
display(df_boot_tear)

# Save to CSV
df_boot_tear.to_csv(f'{base_path}/tear_bootstrap_metrics.csv', index=False)

print("Saved tear bootstrap metrics to CSV.")

# Save in the new .keras format (recommended going forward)
model.save(f'{base_path}/tear_model_converted.keras')

print("Saved converted tear model as tear_model_converted.keras")

import numpy as np
import pandas as pd

base_path = '/content/drive/My Drive/Deep learning model'

# Save to Drive so it's persistent
np.save(f'{base_path}/X_all.npy', X_all)
metadata_valid.to_csv(f'{base_path}/metadata_valid.csv', index=False)

print("Saved X_all.npy and metadata_valid.csv to Drive.")

import numpy as np
import pandas as pd

base_path = '/content/drive/My Drive/Deep learning model'

X_all_test = np.load(f'{base_path}/X_all.npy')
metadata_valid_test = pd.read_csv(f'{base_path}/metadata_valid.csv')

print("X_all_test shape:", X_all_test.shape)
print("metadata_valid_test shape:", metadata_valid_test.shape)
print("Columns:", metadata_valid_test.columns.tolist())

import numpy as np
import pandas as pd
import os

base_path = '/content/drive/My Drive/Deep learning model'

# Check what files are there (optional, just to be sure)
print(os.listdir(base_path))

# Load preprocessed images
X_all = np.load(f'{base_path}/X_all.npy')
print("X_all shape:", X_all.shape)

# Load metadata that matches X_all
metadata = pd.read_csv(f'{base_path}/metadata_valid.csv')
print("metadata shape:", metadata.shape)
print(metadata.columns)

# 1. Check the column exists
assert 'tear_volume' in metadata.columns, "tear_volume column not found in metadata_valid.csv"

# 2. Keep only rows where tear_volume is not null
mask = metadata['tear_volume'].notna()

X_tv = X_all[mask.values]                     # images for tear_volume
y_tv_raw = metadata.loc[mask, 'tear_volume']  # labels for tear_volume

print("X_tv shape:", X_tv.shape)
print("y_tv_raw shape:", y_tv_raw.shape)
print("Unique tear_volume values:", y_tv_raw.unique())

# If they are strings like "0", "1", ... convert to integers
y_tv = y_tv_raw.astype(int).values

print("Classes present:", np.unique(y_tv))
n_classes = len(np.unique(y_tv))
print("Number of classes:", n_classes)

from sklearn.model_selection import train_test_split

# First: train vs temp (val+test)
X_train, X_temp, y_train, y_temp = train_test_split(
    X_tv,
    y_tv,
    test_size=0.30,
    random_state=42,
    stratify=y_tv
)

# Then: split temp into val and test (half-half of 30% → 15/15)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp,
    y_temp,
    test_size=0.50,
    random_state=42,
    stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

from tensorflow.keras.utils import to_categorical

y_train_cat = to_categorical(y_train, num_classes=n_classes)
y_val_cat   = to_categorical(y_val,   num_classes=n_classes)
y_test_cat  = to_categorical(y_test,  num_classes=n_classes)

print("y_train_cat shape:", y_train_cat.shape)
print("y_val_cat shape:", y_val_cat.shape)
print("y_test_cat shape:", y_test_cat.shape)

from tensorflow.keras.models import load_model

model_path = "/content/drive/My Drive/Deep learning model/tear_volume_model.h5"

model = load_model(model_path, compile=False)
model.summary()

y_test_probs = model.predict(X_test)
y_test_pred  = np.argmax(y_test_probs, axis=1)

# Point estimates
test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1       = f1_score(y_test, y_test_pred, average='weighted')
test_kappa    = cohen_kappa_score(y_test, y_test_pred)

# Multi-class AUC (macro, one-vs-rest)
try:
    test_auc = roc_auc_score(
        y_test,
        y_test_probs,
        multi_class='ovr',
        average='macro'
    )
except ValueError:
    test_auc = np.nan  # in case some class is missing

print(f"Test Accuracy: {test_accuracy:.4f}")
print(f"Test F1 (weighted): {test_f1:.4f}")
print(f"Test AUC (macro OVR): {test_auc:.4f}")
print(f"Test Kappa: {test_kappa:.4f}")

from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, roc_auc_score

n_bootstraps = 1000       # you can lower to 200–500 if needed
rng = np.random.RandomState(42)

n_samples = len(y_test)
indices = np.arange(n_samples)

boot_acc   = []
boot_f1    = []
boot_kappa = []
boot_auc   = []

for i in range(n_bootstraps):
    # Sample indices with replacement
    sample_idx = rng.choice(indices, size=n_samples, replace=True)

    y_true_bs = y_test[sample_idx]
    y_pred_bs = y_test_pred[sample_idx]
    y_prob_bs = y_test_probs[sample_idx]

    # Basic metrics
    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs, average='weighted'))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    # AUC can fail if only 1 class is present in the bootstrap sample
    try:
        auc_bs = roc_auc_score(
            y_true_bs,
            y_prob_bs,
            multi_class='ovr',
            average='macro'
        )
        boot_auc.append(auc_bs)
    except ValueError:
        # skip this bootstrap if AUC cannot be computed
        continue

len(boot_acc), len(boot_f1), len(boot_kappa), len(boot_auc)

import numpy as np

def ci_from_bootstrap(samples, alpha=0.95):
    samples = np.array(samples)
    lower = np.percentile(samples, (1 - alpha) / 2 * 100)
    upper = np.percentile(samples, (1 + alpha) / 2 * 100)
    return lower, upper

acc_ci   = ci_from_bootstrap(boot_acc)
f1_ci    = ci_from_bootstrap(boot_f1)
kappa_ci = ci_from_bootstrap(boot_kappa)

print(f"Accuracy: {test_accuracy:.4f} "
      f"(95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")

print(f"F1 (weighted): {test_f1:.4f} "
      f"(95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")

print(f"Kappa: {test_kappa:.4f} "
      f"(95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

# Handle AUC CI separately because boot_auc may be empty
if len(boot_auc) > 0:
    auc_ci = ci_from_bootstrap(boot_auc)
    print(f"AUC (macro OVR): {test_auc:.4f} "
          f"(95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
else:
    print(f"AUC (macro OVR): {test_auc:.4f} "
          "(95% CI: not estimable via bootstrap – "
          "many resamples lacked at least one class)")

from sklearn.metrics import mean_absolute_error

test_mae = mean_absolute_error(y_test, y_test_pred)
print(f"MAE: {test_mae:.4f}")
n_bootstraps = 1000
rng = np.random.RandomState(42)
n_samples = len(y_test)
indices = np.arange(n_samples)

boot_mae = []

for i in range(n_bootstraps):
    sample_idx = rng.choice(indices, size=n_samples, replace=True)

    y_true_bs = y_test[sample_idx]
    y_pred_bs = y_test_pred[sample_idx]

    boot_mae.append(mean_absolute_error(y_true_bs, y_pred_bs))

len(boot_mae)
def ci_from_bootstrap(samples, alpha=0.95):
    samples = np.array(samples)
    lower = np.percentile(samples, (1 - alpha) / 2 * 100)
    upper = np.percentile(samples, (1 + alpha) / 2 * 100)
    return lower, upper

mae_ci = ci_from_bootstrap(boot_mae)

print(f"MAE: {test_mae:.4f} (95% CI: {mae_ci[0]:.4f} – {mae_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

# Load preprocessed images (already normalised etc.)
X_all = np.load(f'{base_path}/X_all.npy')
print("X_all shape:", X_all.shape)

# Load metadata that aligns with X_all
metadata = pd.read_csv(f'{base_path}/metadata_valid.csv')
print("metadata shape:", metadata.shape)
print(metadata.columns)

# Ensure column exists
assert 'pscore' in metadata.columns, "pscore column not found in metadata_valid.csv"

# Keep only rows where pscore is not null
mask = metadata['pscore'].notna()

X_ps = X_all[mask.to_numpy()]                 # images for pscore
y_ps_raw = metadata.loc[mask, 'pscore']       # raw labels

print("X_ps shape:", X_ps.shape)
print("y_ps_raw shape:", y_ps_raw.shape)
print("Unique raw pscore values:", y_ps_raw.unique())

y_ps = y_ps_raw.astype(int).to_numpy()

print("Classes present:", np.unique(y_ps))
n_classes = len(np.unique(y_ps))
print("Number of classes:", n_classes)

from sklearn.model_selection import train_test_split

# Train vs temp (val+test) → 30%
X_train, X_temp, y_train, y_temp = train_test_split(
    X_ps,
    y_ps,
    test_size=0.30,
    random_state=42,
    stratify=y_ps
)

# Temp → val (10%) and test (20%)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp,
    y_temp,
    test_size=(20/30),  # = 0.6667 of the 30% = 20% overall
    random_state=42,
    stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

from tensorflow.keras.models import load_model

model_path = f"{base_path}/pscore_model.h5"
model = load_model(model_path, compile=False)
model.summary()

# Probabilities & hard predictions
y_test_probs = model.predict(X_test)          # shape: (N_test, n_classes)
y_test_pred  = np.argmax(y_test_probs, axis=1)

from sklearn.metrics import accuracy_score, cohen_kappa_score, mean_absolute_error

test_accuracy = accuracy_score(y_test, y_test_pred)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)
test_mae      = mean_absolute_error(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"Kappa:    {test_kappa:.4f}")
print(f"MAE:      {test_mae:.4f}")

import numpy as np

n_bootstraps = 1000
rng = np.random.RandomState(42)
n_samples = len(y_test)
indices = np.arange(n_samples)

boot_acc   = []
boot_mae   = []
boot_kappa = []

for i in range(n_bootstraps):
    sample_idx = rng.choice(indices, size=n_samples, replace=True)

    y_true_bs = y_test[sample_idx]
    y_pred_bs = y_test_pred[sample_idx]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_mae.append(mean_absolute_error(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

def ci_from_bootstrap(samples, alpha=0.95):
    samples = np.array(samples)
    lower = np.percentile(samples, (1 - alpha) / 2 * 100)
    upper = np.percentile(samples, (1 + alpha) / 2 * 100)
    return lower, upper

acc_ci   = ci_from_bootstrap(boot_acc)
mae_ci   = ci_from_bootstrap(boot_mae)
kappa_ci = ci_from_bootstrap(boot_kappa)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"MAE:      {test_mae:.4f} (95% CI: {mae_ci[0]:.4f} – {mae_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

base_path = "/content/drive/My Drive/Deep learning model"

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

X_all = np.load(f"{base_path}/X_all.npy")
print("X_all shape:", X_all.shape)

metadata = pd.read_csv(f"{base_path}/metadata_valid.csv")
print("metadata shape:", metadata.shape)
print(metadata.columns)

assert 'cornea_opacity_visible' in metadata.columns, "Column missing!"

mask = metadata['cornea_opacity_visible'].notna()

X_var = X_all[mask.to_numpy()]
y_var_raw = metadata.loc[mask, 'cornea_opacity_visible']

print("Unique raw values:", y_var_raw.unique())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_var = le.fit_transform(y_var_raw)

print("Encoded classes:", le.classes_)
print("Encoded values:", np.unique(y_var))

from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(
    X_var, y_var, test_size=0.30, random_state=42, stratify=y_var
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(20/30), random_state=42, stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

model_path = "/content/drive/My Drive/Deep learning model/cornea_opacity_visible_model.h5"

base_model = EfficientNetV2B2(
    include_top=False,
    weights='imagenet',        # <<< restore pretrained weights
    input_shape=(224, 224, 3),
    pooling='avg'
)

x = base_model.output
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

model.load_weights(model_path)
model.summary()

y_test_probs = model.predict(X_test).ravel()
y_test_pred = (y_test_probs >= 0.5).astype(int)

from sklearn.metrics import (
    accuracy_score, f1_score, roc_auc_score, cohen_kappa_score
)

test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1       = f1_score(y_test, y_test_pred)
test_auc      = roc_auc_score(y_test, y_test_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"F1:       {test_f1:.4f}")
print(f"AUC:      {test_auc:.4f}")
print(f"Kappa:    {test_kappa:.4f}")

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc  = []
boot_f1   = []
boot_auc  = []
boot_kappa = []

for i in range(n_bootstraps):
    sample_idx = rng.choice(indices, size=len(indices), replace=True)

    y_true_bs = y_test[sample_idx]
    y_pred_bs = y_test_pred[sample_idx]
    y_prob_bs = y_test_probs[sample_idx]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    try:
        auc_bs = roc_auc_score(y_true_bs, y_prob_bs)
        boot_auc.append(auc_bs)
    except:
        continue

def ci(samples):
    return np.percentile(samples, 2.5), np.percentile(samples, 97.5)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc) if len(boot_auc) > 0 else (None, None)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

X_all = np.load(f"{base_path}/X_all.npy")
print("X_all shape:", X_all.shape)

metadata = pd.read_csv(f"{base_path}/metadata_valid.csv")
print("metadata shape:", metadata.shape)
print(metadata.columns)

mask = metadata['cornea_opacitiy_touches_limbus'].notna()

X_var = X_all[mask.to_numpy()]
y_var_raw = metadata.loc[mask, 'cornea_opacitiy_touches_limbus']

print("Raw labels:", y_var_raw.unique())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_var = le.fit_transform(y_var_raw)

print("Encoded classes:", le.classes_)
print("Unique encoded:", np.unique(y_var))

from sklearn.model_selection import train_test_split

# Train vs temp (val+test)
X_train, X_temp, y_train, y_temp = train_test_split(
    X_var, y_var, test_size=0.30, random_state=42, stratify=y_var
)

# Val vs test
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(20/30), random_state=42, stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

import tensorflow as tf
from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

# Path to weights file
model_path = "/content/drive/My Drive/Deep learning model/cornea_opacitiy_touches_limbus_model.h5"

# Build EfficientNet model architecture
input_shape = (224, 224, 3)

base_model = EfficientNetV2B2(
    include_top=False,
    weights='imagenet',         # IMPORTANT
    input_shape=input_shape,
    pooling='avg'
)

x = base_model.output
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

# Load your saved weights
model.load_weights(model_path)

model.summary()

y_test_probs = model.predict(X_test).ravel()
y_test_pred = (y_test_probs >= 0.5).astype(int)

print("Pred classes:", np.unique(y_test_pred, return_counts=True))

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score

test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1       = f1_score(y_test, y_test_pred)
test_auc      = roc_auc_score(y_test, y_test_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"F1:       {test_f1:.4f}")
print(f"AUC:      {test_auc:.4f}")
print(f"Kappa:    {test_kappa:.4f}")

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc  = []
boot_f1   = []
boot_auc  = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)
    y_true_bs = y_test[s]
    y_pred_bs = y_test_pred[s]
    y_prob_bs = y_test_probs[s]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    try:
        boot_auc.append(roc_auc_score(y_true_bs, y_prob_bs))
    except:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

# Load preprocessed images (already normalised etc.)
X_all = np.load(f'{base_path}/X_all.npy')
print("X_all shape:", X_all.shape)

# Load metadata that aligns with X_all
metadata = pd.read_csv(f'{base_path}/metadata_valid.csv')
print("metadata shape:", metadata.shape)
print(metadata.columns)

# Ensure column exists
assert 'cornea_opaqueness' in metadata.columns, "cornea_opaqueness column not found in metadata_valid.csv"

# Keep only rows where pscore is not null
mask = metadata['cornea_opaqueness'].notna()

X_ps = X_all[mask.to_numpy()]                 # images for cornea_opaqueness
y_ps_raw = metadata.loc[mask, 'cornea_opaqueness']       # raw labels

print("X_ps shape:", X_ps.shape)
print("y_ps_raw shape:", y_ps_raw.shape)
print("Unique raw cornea_opaqueness values:", y_ps_raw.unique())

y_ps = y_ps_raw.astype(int).to_numpy()

print("Classes present:", np.unique(y_ps))
n_classes = len(np.unique(y_ps))
print("Number of classes:", n_classes)

from tensorflow.keras.models import load_model

model_path = f"{base_path}/cornea_opaqueness_model.h5"
model = load_model(model_path, compile=False)
model.summary()

# Probabilities & hard predictions
y_test_probs = model.predict(X_test)          # shape: (N_test, n_classes)
y_test_pred  = np.argmax(y_test_probs, axis=1)

from sklearn.metrics import accuracy_score, cohen_kappa_score, mean_absolute_error

test_accuracy = accuracy_score(y_test, y_test_pred)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)
test_mae      = mean_absolute_error(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"Kappa:    {test_kappa:.4f}")
print(f"MAE:      {test_mae:.4f}")

import numpy as np

n_bootstraps = 1000
rng = np.random.RandomState(42)
n_samples = len(y_test)
indices = np.arange(n_samples)

boot_acc   = []
boot_mae   = []
boot_kappa = []

for i in range(n_bootstraps):
    sample_idx = rng.choice(indices, size=n_samples, replace=True)

    y_true_bs = y_test[sample_idx]
    y_pred_bs = y_test_pred[sample_idx]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_mae.append(mean_absolute_error(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

def ci_from_bootstrap(samples, alpha=0.95):
    samples = np.array(samples)
    lower = np.percentile(samples, (1 - alpha) / 2 * 100)
    upper = np.percentile(samples, (1 + alpha) / 2 * 100)
    return lower, upper

acc_ci   = ci_from_bootstrap(boot_acc)
mae_ci   = ci_from_bootstrap(boot_mae)
kappa_ci = ci_from_bootstrap(boot_kappa)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"MAE:      {test_mae:.4f} (95% CI: {mae_ci[0]:.4f} – {mae_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

# Load preprocessed images (already normalised etc.)
X_all = np.load(f'{base_path}/X_all.npy')
print("X_all shape:", X_all.shape)

# Load metadata that aligns with X_all
metadata = pd.read_csv(f'{base_path}/metadata_valid.csv')
print("metadata shape:", metadata.shape)
print(metadata.columns)

# Ensure column exists
assert 'cornea_opacity_size' in metadata.columns, "cornea_opacity_size column not found in metadata_valid.csv"

# Keep only rows where pscore is not null
mask = metadata['cornea_opacity_size'].notna()

X_ps = X_all[mask.to_numpy()]                 # images for pscore
y_ps_raw = metadata.loc[mask, 'cornea_opacity_size']       # raw labels

print("X_ps shape:", X_ps.shape)
print("y_ps_raw shape:", y_ps_raw.shape)
print("Unique raw cornea_opacity_size values:", y_ps_raw.unique())

y_ps = y_ps_raw.astype(int).to_numpy()

print("Classes present:", np.unique(y_ps))
n_classes = len(np.unique(y_ps))
print("Number of classes:", n_classes)

from tensorflow.keras.models import load_model

model_path = f"{base_path}/cornea_opaqueness_model.h5"
model = load_model(model_path, compile=False)
model.summary()

# Probabilities & hard predictions
y_test_probs = model.predict(X_test)          # shape: (N_test, n_classes)
y_test_pred  = np.argmax(y_test_probs, axis=1)

from sklearn.metrics import accuracy_score, cohen_kappa_score, mean_absolute_error

test_accuracy = accuracy_score(y_test, y_test_pred)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)
test_mae      = mean_absolute_error(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"Kappa:    {test_kappa:.4f}")
print(f"MAE:      {test_mae:.4f}")

import numpy as np

n_bootstraps = 1000
rng = np.random.RandomState(42)
n_samples = len(y_test)
indices = np.arange(n_samples)

boot_acc   = []
boot_mae   = []
boot_kappa = []

for i in range(n_bootstraps):
    sample_idx = rng.choice(indices, size=n_samples, replace=True)

    y_true_bs = y_test[sample_idx]
    y_pred_bs = y_test_pred[sample_idx]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_mae.append(mean_absolute_error(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

def ci_from_bootstrap(samples, alpha=0.95):
    samples = np.array(samples)
    lower = np.percentile(samples, (1 - alpha) / 2 * 100)
    upper = np.percentile(samples, (1 + alpha) / 2 * 100)
    return lower, upper

acc_ci   = ci_from_bootstrap(boot_acc)
mae_ci   = ci_from_bootstrap(boot_mae)
kappa_ci = ci_from_bootstrap(boot_kappa)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"MAE:      {test_mae:.4f} (95% CI: {mae_ci[0]:.4f} – {mae_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

X_all = np.load(f"{base_path}/X_all.npy")
print("X_all shape:", X_all.shape)

metadata = pd.read_csv(f"{base_path}/metadata_valid.csv")
print("metadata shape:", metadata.shape)
print(metadata.columns)

assert 'corneal_blood_vessels_tree' in metadata.columns, "Column missing!"

mask = metadata['corneal_blood_vessels_tree'].notna()

X_var = X_all[mask.to_numpy()]
y_var_raw = metadata.loc[mask, 'corneal_blood_vessels_tree']

print("Unique raw values:", y_var_raw.unique())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_var = le.fit_transform(y_var_raw)

print("Encoded classes:", le.classes_)
print("Encoded values:", np.unique(y_var))

from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(
    X_var, y_var, test_size=0.30, random_state=42, stratify=y_var
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(20/30), random_state=42, stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

model_path = "/content/drive/My Drive/Deep learning model/corneal_blood_vessels_tree_model.h5"

base_model = EfficientNetV2B2(
    include_top=False,
    weights='imagenet',        # <<< restore pretrained weights
    input_shape=(224, 224, 3),
    pooling='avg'
)

x = base_model.output
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

model.load_weights(model_path)
model.summary()

y_test_probs = model.predict(X_test).ravel()
y_test_pred = (y_test_probs >= 0.5).astype(int)
print("Pred classes:", np.unique(y_test_pred, return_counts=True))

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score

test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1       = f1_score(y_test, y_test_pred)
test_auc      = roc_auc_score(y_test, y_test_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"F1:       {test_f1:.4f}")
print(f"AUC:      {test_auc:.4f}")
print(f"Kappa:    {test_kappa:.4f}")

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc  = []
boot_f1   = []
boot_auc  = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)
    y_true_bs = y_test[s]
    y_pred_bs = y_test_pred[s]
    y_prob_bs = y_test_probs[s]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    try:
        boot_auc.append(roc_auc_score(y_true_bs, y_prob_bs))
    except:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

X_all = np.load(f"{base_path}/X_all.npy")
print("X_all shape:", X_all.shape)

metadata = pd.read_csv(f"{base_path}/metadata_valid.csv")
print("metadata shape:", metadata.shape)
print(metadata.columns)

assert 'corneal_blood_vessels_across_lesion' in metadata.columns, "Column missing!"

mask = metadata['corneal_blood_vessels_across_lesion'].notna()

X_var = X_all[mask.to_numpy()]
y_var_raw = metadata.loc[mask, 'corneal_blood_vessels_across_lesion']

print("Unique raw values:", y_var_raw.unique())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_var = le.fit_transform(y_var_raw)

print("Encoded classes:", le.classes_)
print("Encoded values:", np.unique(y_var))

from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(
    X_var, y_var, test_size=0.30, random_state=42, stratify=y_var
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(20/30), random_state=42, stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

model_path = "/content/drive/My Drive/Deep learning model/corneal_blood_vessels_across_lesion.h5"

base_model = EfficientNetV2B2(
    include_top=False,
    weights='imagenet',        # <<< restore pretrained weights
    input_shape=(224, 224, 3),
    pooling='avg'
)

x = base_model.output
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

model.load_weights(model_path)
model.summary()

y_test_probs = model.predict(X_test).ravel()
y_test_pred = (y_test_probs >= 0.5).astype(int)
print("Pred classes:", np.unique(y_test_pred, return_counts=True))

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score

test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1       = f1_score(y_test, y_test_pred)
test_auc      = roc_auc_score(y_test, y_test_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"F1:       {test_f1:.4f}")
print(f"AUC:      {test_auc:.4f}")
print(f"Kappa:    {test_kappa:.4f}")

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc  = []
boot_f1   = []
boot_auc  = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)
    y_true_bs = y_test[s]
    y_pred_bs = y_test_pred[s]
    y_prob_bs = y_test_probs[s]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    try:
        boot_auc.append(roc_auc_score(y_true_bs, y_prob_bs))
    except:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

X_all = np.load(f"{base_path}/X_all.npy")
print("X_all shape:", X_all.shape)

metadata = pd.read_csv(f"{base_path}/metadata_valid.csv")
print("metadata shape:", metadata.shape)
print(metadata.columns)

assert 'cornea_opacitiy_touches_limbus' in metadata.columns, "Column missing!"

mask = metadata['cornea_opacitiy_touches_limbus'].notna()

X_var = X_all[mask.to_numpy()]
y_var_raw = metadata.loc[mask, 'cornea_opacitiy_touches_limbus']

print("Unique raw values:", y_var_raw.unique())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_var = le.fit_transform(y_var_raw)

print("Encoded classes:", le.classes_)
print("Encoded values:", np.unique(y_var))

from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(
    X_var, y_var, test_size=0.30, random_state=42, stratify=y_var
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(20/30), random_state=42, stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

model_path = "/content/drive/My Drive/Deep learning model/cornea_opacitiy_touches_limbus.h5"

base_model = EfficientNetV2B2(
    include_top=False,
    weights='imagenet',        # <<< restore pretrained weights
    input_shape=(224, 224, 3),
    pooling='avg'
)

x = base_model.output
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

model.load_weights(model_path)
model.summary()

y_test_probs = model.predict(X_test).ravel()
y_test_pred = (y_test_probs >= 0.5).astype(int)
print("Pred classes:", np.unique(y_test_pred, return_counts=True))

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score

test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1       = f1_score(y_test, y_test_pred)
test_auc      = roc_auc_score(y_test, y_test_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"F1:       {test_f1:.4f}")
print(f"AUC:      {test_auc:.4f}")
print(f"Kappa:    {test_kappa:.4f}")

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc  = []
boot_f1   = []
boot_auc  = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)
    y_true_bs = y_test[s]
    y_pred_bs = y_test_pred[s]
    y_prob_bs = y_test_probs[s]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    try:
        boot_auc.append(roc_auc_score(y_true_bs, y_prob_bs))
    except:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

X_all = np.load(f"{base_path}/X_all.npy")
print("X_all shape:", X_all.shape)

metadata = pd.read_csv(f"{base_path}/metadata_valid.csv")
print("metadata shape:", metadata.shape)
print(metadata.columns)

assert 'corneal_blood_vessels_hedges' in metadata.columns, "Column missing!"

mask = metadata['corneal_blood_vessels_hedges'].notna()

X_var = X_all[mask.to_numpy()]
y_var_raw = metadata.loc[mask, 'corneal_blood_vessels_hedges']

print("Unique raw values:", y_var_raw.unique())

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_var = le.fit_transform(y_var_raw)

print("Encoded classes:", le.classes_)
print("Encoded values:", np.unique(y_var))

from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(
    X_var, y_var, test_size=0.30, random_state=42, stratify=y_var
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(20/30), random_state=42, stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

model_path = "/content/drive/My Drive/Deep learning model/corneal_blood_vessels_hedges.h5"

base_model = EfficientNetV2B2(
    include_top=False,
    weights='imagenet',        # <<< restore pretrained weights
    input_shape=(224, 224, 3),
    pooling='avg'
)

x = base_model.output
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(1, activation='sigmoid')(x)

model = Model(inputs=base_model.input, outputs=output)

model.load_weights(model_path)
model.summary()

y_test_probs = model.predict(X_test).ravel()
y_test_pred = (y_test_probs >= 0.5).astype(int)
print("Pred classes:", np.unique(y_test_pred, return_counts=True))

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, cohen_kappa_score

test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1       = f1_score(y_test, y_test_pred)
test_auc      = roc_auc_score(y_test, y_test_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"F1:       {test_f1:.4f}")
print(f"AUC:      {test_auc:.4f}")
print(f"Kappa:    {test_kappa:.4f}")

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc  = []
boot_f1   = []
boot_auc  = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)
    y_true_bs = y_test[s]
    y_pred_bs = y_test_pred[s]
    y_prob_bs = y_test_probs[s]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    try:
        boot_auc.append(roc_auc_score(y_true_bs, y_prob_bs))
    except:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

import numpy as np
import pandas as pd
import os

print(os.listdir(base_path))

# Load preprocessed images
X_all = np.load(f"{base_path}/X_all.npy")
print("X_all shape:", X_all.shape)

# Load metadata
metadata = pd.read_csv(f"{base_path}/metadata_valid.csv")
print("metadata shape:", metadata.shape)
print(metadata.Columns)

# ----- VARIABLE-SPECIFIC SECTION -----
# Multiclass (non-ordinal): corneal_surface
assert 'corneal_surface' in metadata.columns, "Column 'corneal_surface' missing!"

# Keep rows with non-missing labels
mask = metadata['corneal_surface'].notna()

X_var = X_all[mask.to_numpy()]
y_var_raw = metadata.loc[mask, 'corneal_surface']

print("Unique raw values:", y_var_raw.unique())

# -----------------------------
# Label encoding (multiclass)
# -----------------------------
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_var = le.fit_transform(y_var_raw)

print("Encoded classes:", le.classes_)
print("Encoded values:", np.unique(y_var))

num_classes = len(le.classes_)

# -----------------------------
# Train / Val / Test split (70 / 10 / 20)
# -----------------------------
from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(
    X_var, y_var, test_size=0.30, random_state=42, stratify=y_var
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=(20/30), random_state=42, stratify=y_temp
)

print("Train:", X_train.shape, y_train.shape)
print("Val:  ", X_val.shape, y_val.shape)
print("Test: ", X_test.shape, y_test.shape)

# -----------------------------
# EfficientNetV2B2 model
# -----------------------------
from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.models import Model

# TODO: update this path to your actual saved weights file
model_path = "/content/drive/My Drive/Deep learning model/corneal_surface_model.h5"

base_model = EfficientNetV2B2(
    include_top=False,
    weights='imagenet',        # pretrained weights
    input_shape=(224, 224, 3),
    pooling='avg'
)

x = base_model.output
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)

# Load previously trained weights
model.load_weights(model_path)
model.summary()

# -----------------------------
# Predictions on test set
# -----------------------------
y_test_probs = model.predict(X_test)                    # shape (N, num_classes)
y_test_pred  = np.argmax(y_test_probs, axis=1)          # hard predictions

print("Pred classes:", np.unique(y_test_pred, return_counts=True))

# -----------------------------
# Metrics: Accuracy, F1 (weighted), AUC (weighted OVR), Kappa
# -----------------------------
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    roc_auc_score,
    cohen_kappa_score,
    classification_report
)

test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1       = f1_score(y_test, y_test_pred, average='weighted')
test_kappa    = cohen_kappa_score(y_test, y_test_pred)

# For multiclass AUC, need one-hot ground truth
from tensorflow.keras.utils import to_categorical
y_test_onehot = to_categorical(y_test, num_classes=num_classes)

# Weighted, one-vs-rest multiclass AUC
test_auc = roc_auc_score(
    y_test_onehot,
    y_test_probs,
    multi_class='ovr',
    average='weighted'
)

print(f"Accuracy: {test_accuracy:.4f}")
print(f"F1 (weighted):  {test_f1:.4f}")
print(f"AUC (weighted): {test_auc:.4f}")
print(f"Kappa:          {test_kappa:.4f}")

print("\nClassification report:")
print(classification_report(y_test, y_test_pred, target_names=le.classes_))

# -----------------------------
# Bootstrapped 95% CI
# -----------------------------
n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc   = []
boot_f1    = []
boot_auc   = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)

    y_true_bs_int   = y_test[s]
    y_pred_bs_int   = y_test_pred[s]
    y_prob_bs       = y_test_probs[s]
    y_true_bs_onehot = y_test_onehot[s]

    # Accuracy, F1 (weighted), Kappa
    boot_acc.append(accuracy_score(y_true_bs_int, y_pred_bs_int))
    boot_f1.append(f1_score(y_true_bs_int, y_pred_bs_int, average='weighted'))
    boot_kappa.append(cohen_kappa_score(y_true_bs_int, y_pred_bs_int))

    # AUC (weighted OVR) – sometimes may fail if only one class present in bootstrap sample
    try:
        auc_bs = roc_auc_score(
            y_true_bs_onehot,
            y_prob_bs,
            multi_class='ovr',
            average='weighted'
        )
        boot_auc.append(auc_bs)
    except Exception:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print(f"\nBootstrapped 95% CIs ({n_bootstraps} resamples):")
print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1 (w):   {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC (w):  {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")