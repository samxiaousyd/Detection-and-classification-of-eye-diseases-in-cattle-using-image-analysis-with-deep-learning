# -*- coding: utf-8 -*-
"""InceptionV3 and VGG19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mNx-GcMa_MDVe6kE9rjSUiPHLKZSjlCl
"""

pip install tensorflow pandas numpy matplotlib

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split

import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
import cv2

# Paths to your dataset on Google Drive
csv_path = '/content/drive/My Drive/Deep learning model/Set11score.csv'
image_folder = '/content/drive/My Drive/Deep learning model/image11'

# Function to load and preprocess images
def load_and_preprocess_image(image_name):
    path = os.path.join(image_folder, image_name + '.JPG')

    if not os.path.isfile(path):
        print(f"File '{image_name}.JPG' does not exist at {path}")
        return None

    image = cv2.imread(path)

    if image is None:
        print(f"Error: Could not read image {image_name} at {path}")
        return None

    image = cv2.resize(image, (224, 224))
    return image

# Load metadata from CSV file
metadata = pd.read_csv(csv_path)

# Normalize the image IDs
metadata['Id'] = metadata['Id'].str.replace('.JPG', '', regex=False)
metadata['Id'] = metadata['Id'].str.replace('.JPEG', '', regex=False)

# Load images based on the IDs in the CSV
images = [load_and_preprocess_image(row['Id']) for idx, row in metadata.iterrows()]

# Filter out any 'None' entries
X = [img for img in images if img is not None]

# Correctly extract target values based on loaded images
valid_image_ids = [row['Id'] for idx, row in metadata.iterrows() if load_and_preprocess_image(row['Id']) is not None]
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Stained'].values

print(f"Number of images loaded: {len(X)}")
print(f"Number of target rows: {len(y)}")

# Convert the feature arrays to NumPy arrays of type float32
X = np.array(X).astype(np.float32)

# Ensure labels are in the correct format
y = y.astype(str)

# Split data into training, validation, and test sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Check shapes of the data
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_val: {X_val.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_val: {y_val.shape}")
print(f"Shape of y_test: {y_test.shape}")

# Verify unique values in labels after splitting
print(f"Unique values in y_train: {np.unique(y_train)}")
print(f"Unique values in y_val: {np.unique(y_val)}")
print(f"Unique values in y_test: {np.unique(y_test)}")

# Encode the labels to binary format
label_encoder = LabelEncoder()
y_train_binary = label_encoder.fit_transform(y_train.ravel())
y_val_binary = label_encoder.transform(y_val.ravel())
y_test_binary = label_encoder.transform(y_test.ravel())

# Check unique values in binary labels
print(f"Unique values in y_train_binary: {np.unique(y_train_binary)}")
print(f"Unique values in y_val_binary: {np.unique(y_val_binary)}")
print(f"Unique values in y_test_binary: {np.unique(y_test_binary)}")

from tensorflow.keras import layers, Model
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
import matplotlib.pyplot as plt

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers to retain pre-learned weights
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers on top of the base model
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)  # Dropout layer to avoid overfitting

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)

# Compile the model with binary crossentropy loss
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Summary of the model
model.summary()

# Train the model using binary labels
history = model.fit(
    X_train,
    y_train_binary,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_binary),
    callbacks=[early_stopping]
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test_binary)
print(f"Test accuracy: {test_accuracy}")

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report
import matplotlib.pyplot as plt

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)  # Convert probabilities to binary predictions

# Calculate accuracy
test_accuracy = accuracy_score(y_test_binary, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_binary, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_binary, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_binary, y_test_pred_classes, target_names=['Negative', 'Positive'])
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:  # Ensure history is not empty
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Calculate precision and recall for different thresholds
precisions, recalls, thresholds = precision_recall_curve(y_test_binary, y_test_pred_probs)

# Calculate F1 scores for all thresholds
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)

# Find the threshold that gives the maximum F1 score
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

print(f"Optimal Threshold: {optimal_threshold:.4f}")
print(f"Best F1 Score: {f1_scores[optimal_idx]:.4f}")

# Update 'y' to represent the attribute of interest (e.g., 'Pscore')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'PScore'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multi-class classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for multi-class classification
output = layers.Dense(num_classes, activation='softmax')(x)

# Create and compile the model for multi-class classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_onehot,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_onehot),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Tear')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Tear'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, 2)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, 2)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, 2)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Tear volume')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Tear volume'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multi-class classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for multi-class classification
output = layers.Dense(num_classes, activation='softmax')(x)

# Create and compile the model for multi-class classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_onehot,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_onehot),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Cornea opacity visible')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacity visible'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, 2)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, 2)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, 2)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Replace missing values in 'Cornea opacity colour' with 'null' where 'Cornea opacity visible' is 'no'
metadata.loc[metadata['Cornea opacity visible'] == 'no', 'Cornea opacity colour'] = 'null'

# Update 'y' to represent the attribute of interest (e.g., 'Cornea opacity colour')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacity colour'].values
y = y.astype(str)

# Split data into training and temp sets without stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (including 'null' as a separate class)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)

# Filter validation and test sets to keep only classes present in training
valid_classes = set(y_train)

# Filter validation set
valid_indices_val = [i for i, label in enumerate(y_val) if label in valid_classes]
X_val = X_val[valid_indices_val]
y_val = y_val[valid_indices_val]

# Filter test set
valid_indices_test = [i for i, label in enumerate(y_test) if label in valid_classes]
X_test = X_test[valid_indices_test]
y_test = y_test[valid_indices_test]

# Encode the filtered validation and test labels
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multi-class classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for multi-class classification
output = layers.Dense(num_classes, activation='softmax')(x)

# Create and compile the model for multi-class classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_onehot,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_onehot),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Check the distribution of classes in the test set
class_distribution = np.unique(y_test_encoded, return_counts=True)
print(f"Class distribution in y_test_encoded: {class_distribution}")

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Check if more than one class is present before calculating AUC
if len(np.unique(y_test_encoded)) > 1:
    test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
    print(f"Test AUC (Macro): {test_auc:.4f}")
else:
    print("ROC AUC is not defined as only one class is present in the test set.")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Cornea opacitiy touches limbus')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacitiy touches limbus'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, 2)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, 2)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, 2)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Cornea opaqueness')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opaqueness'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multi-class classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for multi-class classification
output = layers.Dense(num_classes, activation='softmax')(x)

# Create and compile the model for multi-class classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_onehot,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_onehot),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Cornea opacity size')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacity size'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multi-class classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for multi-class classification
output = layers.Dense(num_classes, activation='softmax')(x)

# Create and compile the model for multi-class classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_onehot,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_onehot),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Corneal surface')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal surface'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multi-class classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for multi-class classification
output = layers.Dense(num_classes, activation='softmax')(x)

# Create and compile the model for multi-class classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_onehot,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_onehot),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: hedges')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: hedges'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, 2)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, 2)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, 2)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=20,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: tree')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: tree'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, 2)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, 2)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, 2)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

from collections import Counter
import numpy as np
import pandas as pd

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: across lesion')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: across lesion'].values
y = y.astype(str)

# Set 'Unable to determine' values to null (NaN)
y[y == 'Unable to determine'] = np.nan

# Remove null values from 'X' and 'y'
valid_indices = ~pd.isnull(y)
X = X[valid_indices]
y = y[valid_indices]

# Check the distribution of classes in 'y' after handling nulls
class_counts = Counter(y)
print(f"Class distribution after removing 'Unable to determine': {class_counts}")

# Create a mask to ignore data points with fewer than 2 samples in their class
valid_classes = [cls for cls, count in class_counts.items() if count >= 2]
mask = np.isin(y, valid_classes)

# Apply the mask during splitting
X_train, X_temp, y_train, y_temp = train_test_split(
    X[mask], y[mask], test_size=0.2, random_state=42, stratify=y[mask] if len(valid_classes) > 1 else None
)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, 2)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, 2)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, 2)

# Continue with the rest of your code...
# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: clearning from limbus')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: clearning from limbus'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, 2)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, 2)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, 2)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = (y_test_pred_probs > 0.5).astype(int)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

# Update 'y' to represent the attribute of interest (e.g., 'Globe shape')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Globe shape'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multi-class classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for multi-class classification
output = layers.Dense(num_classes, activation='softmax')(x)

# Create and compile the model for multi-class classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_onehot,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_onehot),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

model.save('/content/drive/My Drive/Deep learning model/your_InceptionV3_model.h5')

from tensorflow.keras.models import load_model

# Load the saved model
model = load_model('/content/drive/My Drive/Deep learning model/your_InceptionV3_model.h5')

# Summary of the loaded model
model.summary()

from tensorflow.keras.applications import VGG19
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import EarlyStopping

# Ensure that X and y have the same number of samples
assert len(X) == len(y), f"Mismatch: X has {len(X)} samples, but y has {len(y)} samples."

# Update 'y' to represent the binary attribute of interest (e.g., 'Stained')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Stained'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched after filtering
valid_indices = ~pd.isnull(y)  # Filter out any nulls in 'y'
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after filtering: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Load the pretrained VGG19 model, excluding the top layers
base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model using binary labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)
print(f"Test accuracy: {test_accuracy:.4f}")

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

#Ensure that X and y have the same number of samples
assert len(X) == len(y), f"Mismatch: X has {len(X)} samples, but y has {len(y)} samples."

# Update 'y' to represent the binary attribute of interest (e.g., 'Tear')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Tear'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched after filtering
valid_indices = ~pd.isnull(y)  # Filter out any nulls in 'y'
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after filtering: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Load the pretrained VGG19 model, excluding the top layers
base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model using binary labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=20,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)
print(f"Test accuracy: {test_accuracy:.4f}")
# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

#Ensure that X and y have the same number of samples
assert len(X) == len(y), f"Mismatch: X has {len(X)} samples, but y has {len(y)} samples."

# Update 'y' to represent the binary attribute of interest (e.g., 'Cornea opacity visible')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacity visible'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched after filtering
valid_indices = ~pd.isnull(y)  # Filter out any nulls in 'y'
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after filtering: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Load the pretrained VGG19 model, excluding the top layers
base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model using binary labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=20,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)
print(f"Test accuracy: {test_accuracy:.4f}")
# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

#Ensure that X and y have the same number of samples
assert len(X) == len(y), f"Mismatch: X has {len(X)} samples, but y has {len(y)} samples."

# Update 'y' to represent the binary attribute of interest (e.g., 'Cornea opacitiy touches limbus')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Cornea opacitiy touches limbus'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched after filtering
valid_indices = ~pd.isnull(y)  # Filter out any nulls in 'y'
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after filtering: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Load the pretrained VGG19 model, excluding the top layers
base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model using binary labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)
print(f"Test accuracy: {test_accuracy:.4f}")
# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

#Ensure that X and y have the same number of samples
assert len(X) == len(y), f"Mismatch: X has {len(X)} samples, but y has {len(y)} samples."

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: hedges')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: hedges'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched after filtering
valid_indices = ~pd.isnull(y)  # Filter out any nulls in 'y'
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after filtering: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Load the pretrained VGG19 model, excluding the top layers
base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model using binary labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=20,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)
print(f"Test accuracy: {test_accuracy:.4f}")
# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

#Ensure that X and y have the same number of samples
assert len(X) == len(y), f"Mismatch: X has {len(X)} samples, but y has {len(y)} samples."

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: tree')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: tree'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched after filtering
valid_indices = ~pd.isnull(y)  # Filter out any nulls in 'y'
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after filtering: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Load the pretrained VGG19 model, excluding the top layers
base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model using binary labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=20,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)
print(f"Test accuracy: {test_accuracy:.4f}")
# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

#Ensure that X and y have the same number of samples
assert len(X) == len(y), f"Mismatch: X has {len(X)} samples, but y has {len(y)} samples."

# Update 'y' to represent the binary attribute of interest (e.g., 'Corneal blood vessels: across lesion')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'Corneal blood vessels: across lesion'].values
y = y.astype(str)

# Ensure 'X' and 'y' are matched after filtering
valid_indices = ~pd.isnull(y)  # Filter out any nulls in 'y'
X = X[valid_indices]
y = y[valid_indices]

# Double-check the matching sizes again
assert len(X) == len(y), f"Mismatch after filtering: X has {len(X)} samples, but y has {len(y)} samples."

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers (0 or 1)
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Load the pretrained VGG19 model, excluding the top layers
base_model = VGG19(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for binary classification
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model for binary classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define the EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model using binary labels
history = model.fit(
    X_train,
    y_train_encoded,  # Use binary labels directly
    epochs=20,
    batch_size=32,
    validation_data=(X_val, y_val_encoded),
    callbacks=[early_stopping]
)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)
print(f"Test accuracy: {test_accuracy:.4f}")
# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes)
print(f"Test F1 Score: {test_f1_score:.4f}")

# Calculate AUC
test_auc = roc_auc_score(y_test_encoded, y_test_pred_probs)
print(f"Test AUC: {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B2  # Use B1, B2, etc., if needed

# Update 'y' to represent the attribute of interest (e.g., 'Pscore')
y = metadata.loc[metadata['Id'].isin(valid_image_ids), 'PScore'].values
y = y.astype(str)

# Split data into training and temp sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Second split (non-stratified) to create validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Encode the labels to integers
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_val_encoded = label_encoder.transform(y_val)
y_test_encoded = label_encoder.transform(y_test)

# Determine the number of classes dynamically
num_classes = len(np.unique(y_train_encoded))

# Convert to one-hot encoded format
y_train_onehot = tf.keras.utils.to_categorical(y_train_encoded, num_classes)
y_val_onehot = tf.keras.utils.to_categorical(y_val_encoded, num_classes)
y_test_onehot = tf.keras.utils.to_categorical(y_test_encoded, num_classes)

# Load the pretrained InceptionV3 model, excluding the top layers
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multi-class classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Modify the output layer for multi-class classification
output = layers.Dense(num_classes, activation='softmax')(x)

# Create and compile the model for multi-class classification
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model using one-hot encoded labels
history = model.fit(
    X_train,
    y_train_onehot,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val_onehot),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)
y_test_pred_classes = np.argmax(y_test_pred_probs, axis=1)

# Calculate accuracy
test_accuracy = accuracy_score(y_test_encoded, y_test_pred_classes)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate F1 score (macro, considering multi-class)
test_f1_score = f1_score(y_test_encoded, y_test_pred_classes, average='macro')
print(f"Test F1 Score (Macro): {test_f1_score:.4f}")

# Calculate AUC (macro, considering multi-class)
test_auc = roc_auc_score(y_test_onehot, y_test_pred_probs, average='macro', multi_class='ovr')
print(f"Test AUC (Macro): {test_auc:.4f}")

# Classification Report
report = classification_report(y_test_encoded, y_test_pred_classes, target_names=label_encoder.classes_)
print(report)

# Plot training & validation accuracy and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('accuracy', []), label='Training Accuracy')
    plt.plot(history.history.get('val_accuracy', []), label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()
else:
    print("No training history available.")

