# -*- coding: utf-8 -*-
"""Chatper 5 Binary and Multiclass DL

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qnxoYhiEAkYKH2QVRBiD_xSj6h9M907z
"""

# Install necessary packages
!pip install tensorflow pandas numpy matplotlib

import pandas as pd
import numpy as np
import os
import cv2
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from tensorflow.keras import layers, regularizers
from tensorflow.keras.utils import to_categorical

from google.colab import drive
drive.mount('/content/drive')
# Paths to your dataset on Google Drive
csv_path = '/content/drive/My Drive/Deep learning model/Pinkeye/Pinkeye.csv'
image_folder = '/content/drive/My Drive/Deep learning model/Cropped'

# Check if CSV and image folder exist
if not os.path.isfile(csv_path):
    raise FileNotFoundError(f"CSV file not found at {csv_path}")
if not os.path.isdir(image_folder):
    raise FileNotFoundError(f"Image folder not found at {image_folder}")

# Load metadata from CSV file
metadata = pd.read_csv(csv_path)

# Function to load and preprocess images
def load_and_preprocess_image(image_name):
    # Ensure the image_name already ends with .JPEG
    if not image_name.endswith('.JPEG'):
        raise ValueError(f"Image name {image_name} does not have the correct .JPEG extension.")

    # Construct the full image path
    path = os.path.join(image_folder, image_name)

    if os.path.isfile(path):
        image = cv2.imread(path)
        if image is not None:
            # Resize the image to 224x224
            image = cv2.resize(image, (224, 224))
            return image
        else:
            print(f"Error: Could not read image {image_name} at {path}")
    else:
        print(f"File '{image_name}' not found in {image_folder}")
    return None

# Example usage: Loop through metadata and preprocess images
images = [load_and_preprocess_image(row['image_id']) for _, row in metadata.iterrows()]

# Check if 'image_id' and 'A1_2' columns exist
if 'image_id' not in metadata.columns or 'A1_2' not in metadata.columns:
    raise KeyError("Required columns 'image_id' and 'A1_2' not found in CSV file.")

# Load images based on the IDs in the CSV
images = [load_and_preprocess_image(row['image_id']) for idx, row in metadata.iterrows()]

# Filter out any 'None' entries and keep valid IDs
valid_images = [(img, row['image_id']) for img, (_, row) in zip(images, metadata.iterrows()) if img is not None]
X, valid_image_ids = zip(*valid_images) if valid_images else ([], [])

# Correctly extract target values based on loaded images
y = metadata.loc[metadata['image_id'].isin(valid_image_ids), 'A1_2'].values

# Convert the feature arrays to NumPy arrays of type float32
X = np.array(X).astype(np.float32)

# Encode the labels to binary format (Yes vs No)
label_encoder = LabelEncoder()
y_binary = label_encoder.fit_transform(['Yes' if label in ['A1', 'A2'] else 'No' for label in y])

# Split data into training, validation, and test sets with stratification
X_train, X_temp, y_train, y_temp = train_test_split(X, y_binary, test_size=0.3, random_state=42, stratify=y_binary)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2 / 3, random_state=42, stratify=y_temp)

# Verify the dataset shapes
print(f"Shape of X_train: {X_train.shape}, Shape of y_train: {y_train.shape}")
print(f"Shape of X_val: {X_val.shape}, Shape of y_val: {y_val.shape}")
print(f"Shape of X_test: {X_test.shape}, Shape of y_test: {y_test.shape}")
print(f"Class distribution in training set: {np.bincount(y_train)}")
print(f"Class distribution in validation set: {np.bincount(y_val)}")
print(f"Class distribution in test set: {np.bincount(y_test)}")

from tensorflow.keras.regularizers import l2

# Train EfficientNetV2B2 for binary classification
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

from sklearn.metrics import (
    accuracy_score,
    f1_score,
    roc_auc_score,
    cohen_kappa_score
)

# Evaluate the model on train, validation, and test sets
def evaluate_and_print_metrics(X, y, dataset_name):
    y_pred_probs = model.predict(X).ravel()
    y_pred_classes = (y_pred_probs > 0.5).astype(int)

    accuracy = accuracy_score(y, y_pred_classes)
    f1       = f1_score(y, y_pred_classes)
    auc      = roc_auc_score(y, y_pred_probs)
    kappa    = cohen_kappa_score(y, y_pred_classes)

    print(f"\nMetrics for {dataset_name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC:     {auc:.4f}")
    print(f"Kappa:   {kappa:.4f}")

    return y_pred_probs, y_pred_classes

# Train set evaluation
y_train_pred_probs, y_train_pred_classes = evaluate_and_print_metrics(
    X_train, y_train, "Train"
)

# Validation set evaluation
y_val_pred_probs, y_val_pred_classes = evaluate_and_print_metrics(
    X_val, y_val, "Validation"
)

# Test set evaluation
y_test_pred_probs, y_test_pred_classes = evaluate_and_print_metrics(
    X_test, y_test, "Test"
)

# ---------------------------------------------------
# Bootstrapped 95% CI on TEST SET (headline metrics)
# ---------------------------------------------------
n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc   = []
boot_f1    = []
boot_auc   = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)

    y_true_bs   = y_test[s]
    y_pred_bs   = y_test_pred_classes[s]
    y_prob_bs   = y_test_pred_probs[s]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    # AUC can fail if only one class is present in this bootstrap sample
    try:
        boot_auc.append(roc_auc_score(y_true_bs, y_prob_bs))
    except Exception:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

# Recompute point estimates on full test set for clarity
test_accuracy = accuracy_score(y_test, y_test_pred_classes)
test_f1       = f1_score(y_test, y_test_pred_classes)
test_auc      = roc_auc_score(y_test, y_test_pred_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred_classes)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print("\nBootstrapped 95% confidence intervals (Test set):")
print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

# ---------------------------------------------------
# Add predictions and probabilities for all splits
# ---------------------------------------------------
results_df = metadata[metadata['image_id'].isin(valid_image_ids)].reset_index(drop=True)
results_df['probabilities'] = np.nan
results_df['predicted_label'] = np.nan
results_df['split'] = "unknown"

# Assign predictions and probabilities to their corresponding splits
split_mappings = {
    "train": (X_train, y_train_pred_probs, y_train_pred_classes),
    "val":   (X_val,   y_val_pred_probs,   y_val_pred_classes),
    "test":  (X_test,  y_test_pred_probs,  y_test_pred_classes),
}

for split_name, (X_split, y_probs, y_classes) in split_mappings.items():
    # Find indices for the current split
    split_indices = results_df[results_df['image_id'].isin(valid_image_ids[:len(X_split)])].index

    # Assign predictions and probabilities
    results_df.loc[split_indices, 'probabilities'] = y_probs
    results_df.loc[split_indices, 'predicted_label'] = y_classes
    results_df.loc[split_indices, 'split'] = split_name

# Save the updated DataFrame to a CSV file
results_df.to_csv('/content/drive/MyDrive/Deep learning model/results_A1_2_binary.csv', index=False)
print("\nResults with predictions and probabilities saved successfully!")

# Function to display the confusion matrix for the test set
def display_test_confusion_matrix(y_true, y_pred):
    conf_matrix = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['No', 'Yes'])
    disp.plot(cmap='Blues')
    plt.title("Confusion Matrix for Test Set")
    plt.show()

# Display confusion matrix for the test set
display_test_confusion_matrix(y_test, y_test_pred_classes)

# Plot training and validation accuracy and loss
def plot_training_history(history):
    if 'accuracy' in history.history and 'val_accuracy' in history.history:
        # Accuracy Plot
        plt.figure(figsize=(10, 5))
        plt.plot(history.history['accuracy'], label='Training Accuracy')
        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Model Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.show()

    if 'loss' in history.history and 'val_loss' in history.history:
        # Loss Plot
        plt.figure(figsize=(10, 5))
        plt.plot(history.history['loss'], label='Training Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.show()

# Plot training history
plot_training_history(history)

# Prepare binary labels for A1, A2, A3 as "Yes" and others as "No"
metadata['A1_3_binary'] = metadata['A1_2'].apply(lambda x: 'Yes' if x in ['A1', 'A2', 'A3'] else 'No')

# Encode the labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(metadata['A1_3_binary'])  # "Yes" = 1, "No" = 0

# Ensure data consistency
X = np.array(X)  # Assuming X contains the preprocessed images
assert len(X) == len(y), "Mismatch between image data and labels!"

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp)



# Load the EfficientNetV2B2 model
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)  # Added L2 regularization
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model
def evaluate_model(X, y, dataset_name):
    y_pred_probs = model.predict(X).ravel()
    y_pred_classes = (y_pred_probs > 0.5).astype(int)

    accuracy = accuracy_score(y, y_pred_classes)
    f1 = f1_score(y, y_pred_classes)
    auc = roc_auc_score(y, y_pred_probs)

    print(f"\nMetrics for {dataset_name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {auc:.4f}")

    return y_pred_probs, y_pred_classes

# Evaluate train, validation, and test sets
y_train_pred_probs, y_train_pred_classes = evaluate_model(X_train, y_train, "Train")
y_val_pred_probs, y_val_pred_classes = evaluate_model(X_val, y_val, "Validation")
y_test_pred_probs, y_test_pred_classes = evaluate_model(X_test, y_test, "Test")

# Confusion matrix for the test set
conf_matrix = confusion_matrix(y_test, y_test_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_encoder.classes_)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix for Test Set")
plt.show()

# -----------------------------------------
# Bootstrapped 95% CI on TEST SET METRICS
# -----------------------------------------
n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc   = []
boot_f1    = []
boot_auc   = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)

    y_true_bs = y_test[s]
    y_pred_bs = y_test_pred_classes[s]
    y_prob_bs = y_test_pred_probs[s]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    try:
        boot_auc.append(roc_auc_score(y_true_bs, y_prob_bs))
    except Exception:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

# Point estimates on full test set
test_accuracy = accuracy_score(y_test, y_test_pred_classes)
test_f1       = f1_score(y_test, y_test_pred_classes)
test_auc      = roc_auc_score(y_test, y_test_pred_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred_classes)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print("\nBootstrapped 95% confidence intervals (Test set):")
print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

# Plot training history (accuracy and loss)
def plot_training_history(history):
    # Accuracy plot
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    # Loss plot
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

plot_training_history(history)

# Save results to CSV
results_df = metadata[metadata['image_id'].isin(valid_image_ids)].reset_index(drop=True)
results_df['A1_3_binary'] = metadata['A1_3_binary']
results_df['predicted_label'] = np.nan
results_df['probabilities'] = np.nan
results_df['split'] = "unknown"

# Assign predictions and probabilities to their corresponding splits
split_mappings = {
    "train": (X_train, y_train_pred_probs, y_train_pred_classes),
    "val":   (X_val,   y_val_pred_probs,   y_val_pred_classes),
    "test":  (X_test,  y_test_pred_probs,  y_test_pred_classes),
}

for split_name, (X_split, y_probs, y_classes) in split_mappings.items():
    split_indices = results_df[results_df['image_id'].isin(valid_image_ids[:len(X_split)])].index
    results_df.loc[split_indices, 'probabilities'] = y_probs
    results_df.loc[split_indices, 'predicted_label'] = y_classes
    results_df.loc[split_indices, 'split'] = split_name

# Save the results DataFrame
results_df.to_csv('/content/drive/MyDrive/Deep learning model/results_A1_3_binary.csv', index=False)
print("\nResults saved successfully!")

# Save the model
model.save('/content/drive/MyDrive/Deep learning model/A1_3_binary_model.h5')
print("\nModel saved successfully!")

# Prepare binary labels for A1-4 as "Yes" and others as "No"
metadata['A1_4_binary'] = metadata['A1_2'].apply(lambda x: 'Yes' if x in ['A1', 'A2', 'A3', 'A4'] else 'No')

# Encode the labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(metadata['A1_4_binary'])  # "Yes" = 1, "No" = 0

# Ensure data consistency
X = np.array(X)  # Assuming X contains the preprocessed images
assert len(X) == len(y), "Mismatch between image data and labels!"

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp)

# Load the EfficientNetV2B2 model
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)  # Added L2 regularisation
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model
def evaluate_model(X, y, dataset_name):
    y_pred_probs = model.predict(X).ravel()
    y_pred_classes = (y_pred_probs > 0.5).astype(int)

    accuracy = accuracy_score(y, y_pred_classes)
    f1 = f1_score(y, y_pred_classes)
    auc = roc_auc_score(y, y_pred_probs)

    print(f"\nMetrics for {dataset_name}:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"AUC: {auc:.4f}")

    return y_pred_probs, y_pred_classes

# Evaluate train, validation, and test sets
y_train_pred_probs, y_train_pred_classes = evaluate_model(X_train, y_train, "Train")
y_val_pred_probs, y_val_pred_classes = evaluate_model(X_val, y_val, "Validation")
y_test_pred_probs, y_test_pred_classes = evaluate_model(X_test, y_test, "Test")

# Confusion matrix for the test set
conf_matrix = confusion_matrix(y_test, y_test_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_encoder.classes_)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix for Test Set")
plt.show()

# -----------------------------------------
# Bootstrapped 95% CI on TEST SET METRICS
# -----------------------------------------
n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test))

boot_acc   = []
boot_f1    = []
boot_auc   = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)

    y_true_bs = y_test[s]
    y_pred_bs = y_test_pred_classes[s]
    y_prob_bs = y_test_pred_probs[s]

    boot_acc.append(accuracy_score(y_true_bs, y_pred_bs))
    boot_f1.append(f1_score(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs))

    try:
        boot_auc.append(roc_auc_score(y_true_bs, y_prob_bs))
    except Exception:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

# Point estimates on full test set
test_accuracy = accuracy_score(y_test, y_test_pred_classes)
test_f1       = f1_score(y_test, y_test_pred_classes)
test_auc      = roc_auc_score(y_test, y_test_pred_probs)
test_kappa    = cohen_kappa_score(y_test, y_test_pred_classes)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print("\nBootstrapped 95% confidence intervals (Test set):")
print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1:       {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC:      {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

# Plot training history (accuracy and loss)
def plot_training_history(history):
    # Accuracy plot
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    # Loss plot
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

plot_training_history(history)

# Save results to CSV
results_df = metadata[metadata['image_id'].isin(valid_image_ids)].reset_index(drop=True)
results_df['A1_4_binary'] = metadata['A1_4_binary']
results_df['predicted_label'] = np.nan
results_df['probabilities'] = np.nan
results_df['split'] = "unknown"

# Encode the stage column to numeric labels
label_encoder = LabelEncoder()
metadata['stage_encoded'] = label_encoder.fit_transform(metadata['stage'])  # Map classes to integers
num_classes = len(label_encoder.classes_)  # Number of unique classes (4 for A, R, S, Normal)

# One-hot encode the labels for multiclass classification
y = to_categorical(metadata['stage_encoded'], num_classes=num_classes)

# Ensure data consistency
X = np.array(X)  # Assuming X contains the preprocessed images
assert len(X) == len(y), "Mismatch between image data and labels!"

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=metadata['stage_encoded']
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp.argmax(axis=1)  # Use y_temp.argmax for stratification
)

# Load the EfficientNetV2B2 model
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers for multiclass classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)  # Added L2 regularization
x = layers.Dropout(0.5)(x)
output = layers.Dense(num_classes, activation='softmax')(x)  # Softmax for multiclass classification

# Create and compile the model
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model
def evaluate_model(X, y, dataset_name):
    y_pred_probs   = model.predict(X)                  # shape (N, num_classes)
    y_pred_classes = np.argmax(y_pred_probs, axis=1)
    y_true_classes = np.argmax(y, axis=1)

    accuracy = accuracy_score(y_true_classes, y_pred_classes)
    f1       = f1_score(y_true_classes, y_pred_classes, average='weighted')
    kappa    = cohen_kappa_score(y_true_classes, y_pred_classes)

    # Multiclass AUC (weighted, one-vs-rest)
    auc = roc_auc_score(
        y,
        y_pred_probs,
        multi_class='ovr',
        average='weighted'
    )

    report = classification_report(
        y_true_classes,
        y_pred_classes,
        target_names=label_encoder.classes_
    )

    print(f"\nMetrics for {dataset_name}:")
    print(f"Accuracy:       {accuracy:.4f}")
    print(f"F1 (weighted):  {f1:.4f}")
    print(f"AUC (weighted): {auc:.4f}")
    print(f"Kappa:          {kappa:.4f}")
    print("\nClassification report:")
    print(report)

    return y_pred_probs, y_pred_classes

# Evaluate train, validation, and test sets
y_train_pred_probs, y_train_pred_classes = evaluate_model(X_train, y_train, "Train")
y_val_pred_probs,   y_val_pred_classes   = evaluate_model(X_val,   y_val,   "Validation")
y_test_pred_probs,  y_test_pred_classes  = evaluate_model(X_test,  y_test,  "Test")

# Confusion matrix for the test set
y_test_true_classes = np.argmax(y_test, axis=1)
conf_matrix = confusion_matrix(y_test_true_classes, y_test_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=label_encoder.classes_)
disp.plot(cmap='Blues')
plt.title("Confusion Matrix for Test Set")
plt.show()

# -----------------------------------------
# Bootstrapped 95% CI on TEST SET METRICS
# -----------------------------------------
n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test_true_classes))

# One-hot test labels for AUC
y_test_onehot = y_test  # already one-hot

boot_acc   = []
boot_f1    = []
boot_auc   = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)

    y_true_bs_int    = y_test_true_classes[s]
    y_pred_bs_int    = y_test_pred_classes[s]
    y_prob_bs        = y_test_pred_probs[s]
    y_true_bs_onehot = y_test_onehot[s]

    # Accuracy, F1 (weighted), Kappa
    boot_acc.append(accuracy_score(y_true_bs_int, y_pred_bs_int))
    boot_f1.append(f1_score(y_true_bs_int, y_pred_bs_int, average='weighted'))
    boot_kappa.append(cohen_kappa_score(y_true_bs_int, y_pred_bs_int))

    # Weighted, one-vs-rest multiclass AUC
    try:
        auc_bs = roc_auc_score(
            y_true_bs_onehot,
            y_prob_bs,
            multi_class='ovr',
            average='weighted'
        )
        boot_auc.append(auc_bs)
    except Exception:
        continue

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

# Point estimates on full test set
test_accuracy = accuracy_score(y_test_true_classes, y_test_pred_classes)
test_f1       = f1_score(y_test_true_classes, y_test_pred_classes, average='weighted')
test_kappa    = cohen_kappa_score(y_test_true_classes, y_test_pred_classes)
test_auc      = roc_auc_score(
    y_test_onehot,
    y_test_pred_probs,
    multi_class='ovr',
    average='weighted'
)

acc_ci   = ci(boot_acc)
f1_ci    = ci(boot_f1)
kappa_ci = ci(boot_kappa)
auc_ci   = ci(boot_auc)

print("\nBootstrapped 95% confidence intervals (Test set):")
print(f"Accuracy:       {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"F1 (weighted):  {test_f1:.4f} (95% CI: {f1_ci[0]:.4f} – {f1_ci[1]:.4f})")
print(f"AUC (weighted): {test_auc:.4f} (95% CI: {auc_ci[0]:.4f} – {auc_ci[1]:.4f})")
print(f"Kappa:          {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

# -----------------------------------------
# Plot training history (accuracy and loss)
# -----------------------------------------
def plot_training_history(history):
    # Accuracy plot
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    # Loss plot
    plt.figure(figsize=(10, 5))
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

plot_training_history(history)

# -----------------------------------------
# Save results to CSV
# -----------------------------------------
results_df = metadata[metadata['image_id'].isin(valid_image_ids)].reset_index(drop=True)
results_df['stage']         = metadata['stage']
results_df['stage_encoded'] = metadata['stage_encoded']
results_df['predicted_label'] = np.nan
results_df['probabilities']   = np.nan
results_df['split']           = "unknown"

# Assign predictions and probabilities to their corresponding splits
split_mappings = {
    "train": (X_train, y_train_pred_probs, y_train_pred_classes),
    "val":   (X_val,   y_val_pred_probs,   y_val_pred_classes),
    "test":  (X_test,  y_test_pred_probs,  y_test_pred_classes),
}

for split_name, (X_split, y_probs, y_classes) in split_mappings.items():
    split_indices = results_df[results_df['image_id'].isin(valid_image_ids[:len(X_split)])].index
    results_df.loc[split_indices, 'probabilities']   = y_probs.max(axis=1)  # or store full vector separately if you prefer
    results_df.loc[split_indices, 'predicted_label'] = y_classes
    results_df.loc[split_indices, 'split']           = split_name

results_df.to_csv('/content/drive/MyDrive/Deep learning model/results_stage_multiclass.csv', index=False)
print("\nResults saved successfully!")



# Calculate OvR AUC for a given dataset
def calculate_ovr_auc(y_true, y_pred_probs, num_classes):
    auc_scores = []
    for i in range(num_classes):
        class_true = y_true[:, i]  # True labels for the current class
        class_pred = y_pred_probs[:, i]  # Predicted probabilities for the current class
        # Check if there are positive and negative samples for the class
        if np.unique(class_true).size > 1:
            auc = roc_auc_score(class_true, class_pred)
            auc_scores.append(auc)
        else:
            print(f"Skipping AUC for class {i} due to lack of positive/negative samples.")
            auc_scores.append(None)  # Placeholder for skipped class

    # Calculate the average AUC, excluding skipped classes
    valid_auc_scores = [score for score in auc_scores if score is not None]
    average_auc = np.mean(valid_auc_scores) if valid_auc_scores else None
    return average_auc, auc_scores

# Training AUC
train_auc, train_auc_scores = calculate_ovr_auc(y_train, y_train_pred_probs, num_classes)
print(f"Training One-vs-Rest AUC: {train_auc:.4f}")

# Validation AUC
val_auc, val_auc_scores = calculate_ovr_auc(y_val, y_val_pred_probs, num_classes)
print(f"Validation One-vs-Rest AUC: {val_auc:.4f}")

# Test AUC
test_auc, test_auc_scores = calculate_ovr_auc(y_test, y_test_pred_probs, num_classes)
print(f"Test One-vs-Rest AUC: {test_auc:.4f}")

# Encode "Normal" as 0 and everything else as 1 in a new column
metadata['normal_vs_all'] = metadata['stage'].apply(lambda x: 0 if x == 'Normal' else 1)

# Extract features and labels
y = metadata['normal_vs_all'].values
X = np.array(X)  # Assuming X contains the preprocessed images

# Stratified train, validation, and test split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp)

# Define the EfficientNetV2B2 model
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
for layer in base_model.layers:
    layer.trainable = False  # Freeze base model layers

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model
def evaluate_model(X, y, dataset_name):
    y_pred_probs = model.predict(X).ravel()
    y_pred_classes = (y_pred_probs > 0.5).astype(int)

    accuracy = accuracy_score(y, y_pred_classes)
    f1 = f1_score(y, y_pred_classes)
    auc = roc_auc_score(y, y_pred_probs)

    print(f"{dataset_name} Metrics:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  F1 Score: {f1:.4f}")
    print(f"  AUC: {auc:.4f}")
    return y_pred_probs, y_pred_classes, accuracy, f1, auc

# Training set evaluation
y_train_pred_probs, y_train_pred_classes, train_accuracy, train_f1, train_auc = evaluate_model(X_train, y_train, "Training")

# Validation set evaluation
y_val_pred_probs, y_val_pred_classes, val_accuracy, val_f1, val_auc = evaluate_model(X_val, y_val, "Validation")

# Test set evaluation
y_test_pred_probs, y_test_pred_classes, test_accuracy, test_f1, test_auc = evaluate_model(X_test, y_test, "Test")

# Generate confusion matrix for test set
conf_matrix = confusion_matrix(y_test, y_test_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Normal (0)', 'Affected (1)'])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix for Normal vs Everything Else")
plt.show()

# Plot training & validation accuracy and loss
if history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Save results and model
results_df = metadata.copy()
results_df['probabilities'] = model.predict(X).ravel()
results_df['predicted_label'] = (results_df['probabilities'] > 0.5).astype(int)
results_df.to_csv('/content/drive/MyDrive/Deep learning model/results_normal_vs_all.csv', index=False)

model.save('/content/drive/MyDrive/Deep learning model/normal_vs_all_model.h5')

print("Results and model saved successfully!")

# Filter the metadata to include only "A", "R", and "Normal"
filtered_metadata = metadata[metadata['stage'].isin(['A', 'R', 'Normal'])].copy()

# Normalize image IDs in both metadata and valid_image_ids
filtered_metadata['image_id'] = filtered_metadata['image_id'].str.strip().str.lower()
valid_image_ids = [img_id.strip().lower() for img_id in valid_image_ids]

# Encode "A" and "R" as 1 (affected), "Normal" as 0 (healthy)
filtered_metadata['A_R_vs_Normal'] = filtered_metadata['stage'].apply(lambda x: 1 if x in ['A', 'R'] else 0)

# Extract features and labels
filtered_image_ids = filtered_metadata['image_id'].values
X_filtered = np.array([img for img, img_id in zip(X, valid_image_ids) if img_id in filtered_image_ids])
y = filtered_metadata['A_R_vs_Normal'].values

# Debugging: Check consistency
assert len(X_filtered) == len(y), f"Inconsistent data: {len(X_filtered)} images and {len(y)} labels."

# Stratified train, validation, and test split
X_train, X_temp, y_train, y_temp = train_test_split(X_filtered, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=2/3, random_state=42, stratify=y_temp)

# Verify shapes of the splits
print(f"Training set: {X_train.shape}, {y_train.shape}")
print(f"Validation set: {X_val.shape}, {y_val.shape}")
print(f"Test set: {X_test.shape}, {y_test.shape}")

# Define the EfficientNetV2B2 model
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
for layer in base_model.layers:
    layer.trainable = False  # Freeze base model layers

# Add custom layers for binary classification
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
output = layers.Dense(1, activation='sigmoid')(x)

# Create and compile the model
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]
)

# Evaluate the model
def evaluate_model(X, y, dataset_name):
    y_pred_probs = model.predict(X).ravel()
    y_pred_classes = (y_pred_probs > 0.5).astype(int)

    accuracy = accuracy_score(y, y_pred_classes)
    f1 = f1_score(y, y_pred_classes)
    auc = roc_auc_score(y, y_pred_probs)

    print(f"{dataset_name} Metrics:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  F1 Score: {f1:.4f}")
    print(f"  AUC: {auc:.4f}")
    return y_pred_probs, y_pred_classes, accuracy, f1, auc

# Training set evaluation
evaluate_model(X_train, y_train, "Training")

# Validation set evaluation
evaluate_model(X_val, y_val, "Validation")

# Test set evaluation
y_test_pred_probs, y_test_pred_classes, test_accuracy, test_f1, test_auc = evaluate_model(X_test, y_test, "Test")

# Generate confusion matrix for test set
conf_matrix = confusion_matrix(y_test, y_test_pred_classes)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Normal (0)', 'A/R (1)'])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix for A/R vs Normal")
plt.show()

# Plot training & validation accuracy and loss
if history.history:
    plt.figure(figsize=(14, 5))

    # Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Save results and model
filtered_metadata['probabilities'] = model.predict(X).ravel()
filtered_metadata['predicted_label'] = (filtered_metadata['probabilities'] > 0.5).astype(int)
filtered_metadata.to_csv('/content/drive/MyDrive/Deep learning model/results_A_R_vs_Normal.csv', index=False)

model.save('/content/drive/MyDrive/Deep learning model/A_R_vs_Normal_model.h5')

print("Results and model saved successfully!")