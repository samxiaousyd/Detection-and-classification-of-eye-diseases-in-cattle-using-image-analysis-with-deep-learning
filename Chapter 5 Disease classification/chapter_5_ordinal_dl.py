# -*- coding: utf-8 -*-
"""Chapter 5 Ordinal DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qBx8XrY_2l9QEeiN5YwTJ7gStBo9yJ2o
"""

pip install tensorflow pandas numpy matplotlib

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import os
import cv2
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.applications import EfficientNetV2B2
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_absolute_error, mean_squared_error, cohen_kappa_score
from sklearn.utils.class_weight import compute_class_weight

# Paths to your dataset on Google Drive
csv_path = '/content/drive/My Drive/Deep learning model/Pinkeye/Active.csv'
image_folder = '/content/drive/My Drive/Deep learning model/Cropped'

# Check if CSV and image folder exist
if not os.path.isfile(csv_path):
    raise FileNotFoundError(f"CSV file not found at {csv_path}")
if not os.path.isdir(image_folder):
    raise FileNotFoundError(f"Image folder not found at {image_folder}")

# Load metadata from CSV file
metadata = pd.read_csv(csv_path)

# Function to load and preprocess images
def load_and_preprocess_image(image_name):
    # Ensure the image_name already ends with .JPEG
    if not image_name.endswith('.JPEG'):
        raise ValueError(f"Image name {image_name} does not have the correct .JPEG extension.")

    # Construct the full image path
    path = os.path.join(image_folder, image_name)

    if os.path.isfile(path):
        image = cv2.imread(path)
        if image is not None:
            # Resize the image to 224x224
            image = cv2.resize(image, (224, 224))
            return image
        else:
            print(f"Error: Could not read image {image_name} at {path}")
    else:
        print(f"File '{image_name}' not found in {image_folder}")
    return None

# Example usage: Loop through metadata and preprocess images
images = [load_and_preprocess_image(row['image_id']) for _, row in metadata.iterrows()]

# Ensure 'severity' is numeric
metadata['severity'] = pd.to_numeric(metadata['severity'], errors='coerce')
if metadata['severity'].isnull().any():
    raise ValueError("Non-numeric values found in 'severity' column.")

# Extract corresponding labels
y = metadata.loc[metadata['image_id'].isin(metadata['image_id']), 'severity'].values

# Convert the list of images to a NumPy array for training
X = np.array([img for img in images if img is not None])

# Ensure the number of images matches the number of labels
if len(X) != len(y):
    raise ValueError("The number of images does not match the number of labels.")

# Split data into training, validation, and test sets (70-10-20 split)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=2 / 3, random_state=42, stratify=y_temp
)

# Calculate class weights for handling class imbalance
class_weights = compute_class_weight(
    class_weight='balanced', classes=np.unique(y_train), y=y_train
)
class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}

# Convert ordinal labels to cumulative binary labels
def ordinal_to_cumulative_binary(y, num_classes=4):
    return np.array([[1 if i <= label else 0 for i in range(num_classes)] for label in y])

# Apply transformation to the datasets
num_classes = 4  # Set to 4 for pscore
y_train = ordinal_to_cumulative_binary(y_train, num_classes=num_classes)
y_val = ordinal_to_cumulative_binary(y_val, num_classes=num_classes)
y_test = ordinal_to_cumulative_binary(y_test, num_classes=num_classes)

# Load EfficientNet base model
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Custom layers for ordinal regression
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Output layer with 4 units for the ordinal thresholds
output = layers.Dense(4, activation='sigmoid')(x)  # 5 thresholds for 6 categories

# Create the model
model = Model(inputs=base_model.input, outputs=output)

# Custom ordinal loss function (example using mean squared error)
def ordinal_cross_entropy_loss(y_true, y_pred):
    # Clip predictions to avoid log(0) errors
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    # Calculate binary cross-entropy loss
    loss = -tf.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
    return loss


# Compile the model
model.compile(optimizer='adam', loss=ordinal_cross_entropy_loss, metrics=['mae'])

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    class_weight=class_weights_dict,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)

# Convert cumulative binary labels back to ordinal classes
def cumulative_to_ordinal(y):
    return np.sum(y, axis=1)

# Convert predictions to ordinal classes
y_train_pred_classes = np.sum(model.predict(X_train) > 0.5, axis=1)
y_val_pred_classes = np.sum(model.predict(X_val) > 0.5, axis=1)
y_test_pred_classes = np.sum(model.predict(X_test) > 0.5, axis=1)

# Convert true labels to ordinal classes
y_train_classes = cumulative_to_ordinal(y_train)
y_val_classes = cumulative_to_ordinal(y_val)
y_test_classes = cumulative_to_ordinal(y_test)

# Calculate MAE and MSE
train_mae = mean_absolute_error(y_train_classes, y_train_pred_classes)
val_mae = mean_absolute_error(y_val_classes, y_val_pred_classes)
test_mae = mean_absolute_error(y_test_classes, y_test_pred_classes)
train_mse = mean_squared_error(y_train_classes, y_train_pred_classes)
val_mse = mean_squared_error(y_val_classes, y_val_pred_classes)
test_mse = mean_squared_error(y_test_classes, y_test_pred_classes)

# Print metrics
print(f"Training MAE: {train_mae:.4f}, Validation MAE: {val_mae:.4f}, Test MAE: {test_mae:.4f}")
print(f"Training MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}, Test MSE: {test_mse:.4f}")

# Calculate accuracy
train_accuracy = np.mean(y_train_classes == y_train_pred_classes)
val_accuracy = np.mean(y_val_classes == y_val_pred_classes)
test_accuracy = np.mean(y_test_classes == y_test_pred_classes)

print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Validation Accuracy: {val_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Calculate Weighted Kappa
train_kappa = cohen_kappa_score(y_train_classes, y_train_pred_classes, weights='quadratic')
val_kappa   = cohen_kappa_score(y_val_classes, y_val_pred_classes, weights='quadratic')
test_kappa  = cohen_kappa_score(y_test_classes, y_test_pred_classes, weights='quadratic')

print(f"Training Weighted Kappa: {train_kappa:.4f}")
print(f"Validation Weighted Kappa: {val_kappa:.4f}")
print(f"Test Weighted Kappa: {test_kappa:.4f}")

# -------------------------------------------------------------------------
# BOOTSTRAPPING CI
# -------------------------------------------------------------------------

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test_classes))

boot_acc   = []
boot_mae   = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)

    y_true_bs = y_test_classes[s]
    y_pred_bs = y_test_pred_classes[s]

    boot_acc.append(np.mean(y_true_bs == y_pred_bs))
    boot_mae.append(mean_absolute_error(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs, weights='quadratic'))

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
mae_ci   = ci(boot_mae)
kappa_ci = ci(boot_kappa)

print("\nBootstrapped 95% Confidence Intervals (Test set):")
print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"MAE:      {test_mae:.4f} (95% CI: {mae_ci[0]:.4f} – {mae_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

# Define labels for the categories
labels = np.arange(1, 5)  # [1, 2, 3, 4]
class_labels = ['A1', 'A2', 'A3', 'A4']

# Ensure all categories are represented in the confusion matrix
conf_matrix = confusion_matrix(y_test_classes, y_test_pred_classes, labels=labels)

# Create a distance matrix for the ordinal categories
num_classes = len(labels)
distance_matrix = np.abs(np.arange(num_classes).reshape(-1, 1) - np.arange(num_classes))

# Calculate the weighted confusion matrix
weighted_conf_matrix = conf_matrix * (1 - distance_matrix / (num_classes - 1))

# Normalise the confusion matrix for better interpretability
normalised_conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1, keepdims=True)
weighted_normalised_conf_matrix = weighted_conf_matrix / weighted_conf_matrix.sum(axis=1, keepdims=True)

# Plot the weighted normalised confusion matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    weighted_normalised_conf_matrix,
    annot=True,
    fmt=".2f",
    cmap="Blues",
    xticklabels=class_labels,
    yticklabels=class_labels
)
plt.title("Weighted Normalised Confusion Matrix for Ordinal Categories")
plt.xlabel("Predicted Category")
plt.ylabel("True Category")
plt.show()

# Plot training & validation MAE and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # MAE
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('mae', []), label='Training MAE')
    plt.plot(history.history.get('val_mae', []), label='Validation MAE')
    plt.title('Model MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Define labels for the categories
labels = np.arange(1, 5)  # [1, 2, 3, 4]
class_labels = ['A1', 'A2', 'A3', 'A4']
cm = confusion_matrix(y_test_classes, y_test_pred_classes, labels=labels)

# Normalise the confusion matrix (row-wise)
cm_normalised = cm.astype('float') / cm.sum(axis=1, keepdims=True)

# Plot raw confusion matrix
def plot_confusion_matrix(cm, labels, title="Confusion Matrix", normalised=False):
    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm, annot=True, fmt=".2f" if normalised else "d", cmap="Blues",
        xticklabels=[f'R{i}' for i in labels], yticklabels=[f'A{i}' for i in labels]
    )
    plt.title(title)
    plt.xlabel("Predicted Category")
    plt.ylabel("True Category")
    plt.show()

# Plot the raw confusion matrix
plot_confusion_matrix(cm, labels, title="Raw Confusion Matrix")

# Plot the normalised confusion matrix
plot_confusion_matrix(cm_normalised, labels, title="Normalised Confusion Matrix", normalised=True)

# Check class distribution in the test set
unique, counts = np.unique(y_test_classes, return_counts=True)
class_distribution = dict(zip(unique, counts))
print("Class distribution in the test set:", class_distribution)

# Paths to your dataset on Google Drive
csv_path = '/content/drive/My Drive/Deep learning model/Pinkeye/Resolving.csv'
image_folder = '/content/drive/My Drive/Deep learning model/Cropped'

# Check if CSV and image folder exist
if not os.path.isfile(csv_path):
    raise FileNotFoundError(f"CSV file not found at {csv_path}")
if not os.path.isdir(image_folder):
    raise FileNotFoundError(f"Image folder not found at {image_folder}")

# Load metadata from CSV file
metadata = pd.read_csv(csv_path)

# Function to load and preprocess images
def load_and_preprocess_image(image_name):
    # Ensure the image_name already ends with .JPEG
    if not image_name.endswith('.JPEG'):
        raise ValueError(f"Image name {image_name} does not have the correct .JPEG extension.")

    # Construct the full image path
    path = os.path.join(image_folder, image_name)

    if os.path.isfile(path):
        image = cv2.imread(path)
        if image is not None:
            # Resize the image to 224x224
            image = cv2.resize(image, (224, 224))
            return image
        else:
            print(f"Error: Could not read image {image_name} at {path}")
    else:
        print(f"File '{image_name}' not found in {image_folder}")
    return None

# Example usage: Loop through metadata and preprocess images
images = [load_and_preprocess_image(row['image_id']) for _, row in metadata.iterrows()]

# Ensure 'severity' is numeric
metadata['severity'] = pd.to_numeric(metadata['severity'], errors='coerce')
if metadata['severity'].isnull().any():
    raise ValueError("Non-numeric values found in 'severity' column.")

# Extract corresponding labels
y = metadata.loc[metadata['image_id'].isin(metadata['image_id']), 'severity'].values

# Convert the list of images to a NumPy array for training
X = np.array([img for img in images if img is not None])

# Ensure the number of images matches the number of labels
if len(X) != len(y):
    raise ValueError("The number of images does not match the number of labels.")

# Split data into training, validation, and test sets (70-10-20 split)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=2 / 3, random_state=42, stratify=y_temp
)

# Calculate class weights for handling class imbalance
class_weights = compute_class_weight(
    class_weight='balanced', classes=np.unique(y_train), y=y_train
)
class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}

# Convert ordinal labels to cumulative binary labels
def ordinal_to_cumulative_binary(y, num_classes=4):
    return np.array([[1 if i <= label else 0 for i in range(num_classes)] for label in y])

# Apply transformation to the datasets
num_classes = 4  # Set to 4 for pscore
y_train = ordinal_to_cumulative_binary(y_train, num_classes=num_classes)
y_val = ordinal_to_cumulative_binary(y_val, num_classes=num_classes)
y_test = ordinal_to_cumulative_binary(y_test, num_classes=num_classes)

# Load EfficientNet base model
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Custom layers for ordinal regression
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Output layer with 4 units for the ordinal thresholds
output = layers.Dense(4, activation='sigmoid')(x)  # 5 thresholds for 6 categories

# Create the model
model = Model(inputs=base_model.input, outputs=output)

# Custom ordinal loss function (example using mean squared error)
def ordinal_cross_entropy_loss(y_true, y_pred):
    # Clip predictions to avoid log(0) errors
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    # Calculate binary cross-entropy loss
    loss = -tf.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
    return loss

learning_rate = 1e-4

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=learning_rate),
    loss=ordinal_cross_entropy_loss,
    metrics=['mae']
)

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    class_weight=class_weights_dict,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)

# Convert cumulative binary labels back to ordinal classes
def cumulative_to_ordinal(y):
    return np.sum(y, axis=1)

# Convert predictions to ordinal classes
y_train_pred_classes = np.sum(model.predict(X_train) > 0.5, axis=1)
y_val_pred_classes = np.sum(model.predict(X_val) > 0.5, axis=1)
y_test_pred_classes = np.sum(model.predict(X_test) > 0.5, axis=1)

# Convert true labels to ordinal classes
y_train_classes = cumulative_to_ordinal(y_train)
y_val_classes = cumulative_to_ordinal(y_val)
y_test_classes = cumulative_to_ordinal(y_test)

# Calculate MAE and MSE
train_mae = mean_absolute_error(y_train_classes, y_train_pred_classes)
val_mae = mean_absolute_error(y_val_classes, y_val_pred_classes)
test_mae = mean_absolute_error(y_test_classes, y_test_pred_classes)
train_mse = mean_squared_error(y_train_classes, y_train_pred_classes)
val_mse = mean_squared_error(y_val_classes, y_val_pred_classes)
test_mse = mean_squared_error(y_test_classes, y_test_pred_classes)

# Print metrics
print(f"Training MAE: {train_mae:.4f}, Validation MAE: {val_mae:.4f}, Test MAE: {test_mae:.4f}")
print(f"Training MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}, Test MSE: {test_mse:.4f}")

# Calculate accuracy
train_accuracy = np.mean(y_train_classes == y_train_pred_classes)
val_accuracy = np.mean(y_val_classes == y_val_pred_classes)
test_accuracy = np.mean(y_test_classes == y_test_pred_classes)

print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Validation Accuracy: {val_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")


# Calculate Weighted Kappa for training, validation, and test sets
train_kappa = cohen_kappa_score(y_train_classes, y_train_pred_classes, weights='quadratic')
val_kappa = cohen_kappa_score(y_val_classes, y_val_pred_classes, weights='quadratic')
test_kappa = cohen_kappa_score(y_test_classes, y_test_pred_classes, weights='quadratic')

# Print the Kappa scores
print(f"Training Weighted Kappa: {train_kappa:.4f}")
print(f"Validation Weighted Kappa: {val_kappa:.4f}")
print(f"Test Weighted Kappa: {test_kappa:.4f}")

# =========================================================
# Bootstrapped 95% CI
# =========================================================

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test_classes))

boot_acc   = []
boot_mae   = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)
    y_true_bs = y_test_classes[s]
    y_pred_bs = y_test_pred_classes[s]

    boot_acc.append(np.mean(y_true_bs == y_pred_bs))
    boot_mae.append(mean_absolute_error(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs, weights='quadratic'))

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
mae_ci   = ci(boot_mae)
kappa_ci = ci(boot_kappa)

print("\nBootstrapped 95% Confidence Intervals (R-only Test set):")
print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"MAE:      {test_mae:.4f} (95% CI: {mae_ci[0]:.4f} – {mae_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

# Define labels for the categories
labels = np.arange(1, 5)  # [1, 2, 3, 4]
class_labels = ['R1', 'R2', 'R3', 'R4']

# Ensure all categories are represented in the confusion matrix
conf_matrix = confusion_matrix(y_test_classes, y_test_pred_classes, labels=labels)

# Create a distance matrix for the ordinal categories
num_classes = len(labels)
distance_matrix = np.abs(np.arange(num_classes).reshape(-1, 1) - np.arange(num_classes))

# Calculate the weighted confusion matrix
weighted_conf_matrix = conf_matrix * (1 - distance_matrix / (num_classes - 1))

# Normalise the confusion matrix for better interpretability
normalised_conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1, keepdims=True)
weighted_normalised_conf_matrix = weighted_conf_matrix / weighted_conf_matrix.sum(axis=1, keepdims=True)

# Plot the weighted normalised confusion matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    weighted_normalised_conf_matrix,
    annot=True,
    fmt=".2f",
    cmap="Blues",
    xticklabels=class_labels,
    yticklabels=class_labels
)
plt.title("Weighted Normalised Confusion Matrix for Ordinal Categories")
plt.xlabel("Predicted Category")
plt.ylabel("True Category")
plt.show()

# Plot training & validation MAE and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # MAE
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('mae', []), label='Training MAE')
    plt.plot(history.history.get('val_mae', []), label='Validation MAE')
    plt.title('Model MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Paths to your dataset on Google Drive
csv_path = '/content/drive/My Drive/Deep learning model/Pinkeye/Resolved.csv'
image_folder = '/content/drive/My Drive/Deep learning model/Cropped'

# Check if CSV and image folder exist
if not os.path.isfile(csv_path):
    raise FileNotFoundError(f"CSV file not found at {csv_path}")
if not os.path.isdir(image_folder):
    raise FileNotFoundError(f"Image folder not found at {image_folder}")

# Load metadata from CSV file
metadata = pd.read_csv(csv_path)

# Function to load and preprocess images
def load_and_preprocess_image(image_name):
    # Ensure the image_name already ends with .JPEG
    if not image_name.endswith('.JPEG'):
        raise ValueError(f"Image name {image_name} does not have the correct .JPEG extension.")

    # Construct the full image path
    path = os.path.join(image_folder, image_name)

    if os.path.isfile(path):
        image = cv2.imread(path)
        if image is not None:
            # Resize the image to 224x224
            image = cv2.resize(image, (224, 224))
            return image
        else:
            print(f"Error: Could not read image {image_name} at {path}")
    else:
        print(f"File '{image_name}' not found in {image_folder}")
    return None

# Example usage: Loop through metadata and preprocess images
images = [load_and_preprocess_image(row['image_id']) for _, row in metadata.iterrows()]

# Ensure 'severity' is numeric
metadata['severity'] = pd.to_numeric(metadata['severity'], errors='coerce')
if metadata['severity'].isnull().any():
    raise ValueError("Non-numeric values found in 'severity' column.")

# Extract corresponding labels
y = metadata.loc[metadata['image_id'].isin(metadata['image_id']), 'severity'].values

# Convert the list of images to a NumPy array for training
X = np.array([img for img in images if img is not None])

# Ensure the number of images matches the number of labels
if len(X) != len(y):
    raise ValueError("The number of images does not match the number of labels.")

# Split data into training, validation, and test sets (70-10-20 split)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=2 / 3, random_state=42, stratify=y_temp
)

# Calculate class weights for handling class imbalance
class_weights = compute_class_weight(
    class_weight='balanced', classes=np.unique(y_train), y=y_train
)
class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}

# Convert ordinal labels to cumulative binary labels
def ordinal_to_cumulative_binary(y, num_classes=4):
    return np.array([[1 if i <= label else 0 for i in range(num_classes)] for label in y])

# Apply transformation to the datasets
num_classes = 4  # Set to 4 for pscore
y_train = ordinal_to_cumulative_binary(y_train, num_classes=num_classes)
y_val = ordinal_to_cumulative_binary(y_val, num_classes=num_classes)
y_test = ordinal_to_cumulative_binary(y_test, num_classes=num_classes)

# Load EfficientNet base model
base_model = EfficientNetV2B2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Custom layers for ordinal regression
x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)

# Output layer with 4 units for the ordinal thresholds
output = layers.Dense(4, activation='sigmoid')(x)  # 5 thresholds for 6 categories

# Create the model
model = Model(inputs=base_model.input, outputs=output)

# Custom ordinal loss function (example using mean squared error)
def ordinal_cross_entropy_loss(y_true, y_pred):
    # Clip predictions to avoid log(0) errors
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    # Calculate binary cross-entropy loss
    loss = -tf.reduce_mean(y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
    return loss


# Compile the model
model.compile(optimizer='adam', loss=ordinal_cross_entropy_loss, metrics=['mae'])

# Train the model
history = model.fit(
    X_train,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_val, y_val),
    class_weight=class_weights_dict,
    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],
)

# Evaluate the model on the test data
y_test_pred_probs = model.predict(X_test)

# Convert cumulative binary labels back to ordinal classes
def cumulative_to_ordinal(y):
    return np.sum(y, axis=1)

# Convert predictions to ordinal classes
y_train_pred_classes = np.sum(model.predict(X_train) > 0.5, axis=1)
y_val_pred_classes = np.sum(model.predict(X_val) > 0.5, axis=1)
y_test_pred_classes = np.sum(model.predict(X_test) > 0.5, axis=1)

# Convert true labels to ordinal classes
y_train_classes = cumulative_to_ordinal(y_train)
y_val_classes = cumulative_to_ordinal(y_val)
y_test_classes = cumulative_to_ordinal(y_test)

# Calculate MAE and MSE
train_mae = mean_absolute_error(y_train_classes, y_train_pred_classes)
val_mae = mean_absolute_error(y_val_classes, y_val_pred_classes)
test_mae = mean_absolute_error(y_test_classes, y_test_pred_classes)
train_mse = mean_squared_error(y_train_classes, y_train_pred_classes)
val_mse = mean_squared_error(y_val_classes, y_val_pred_classes)
test_mse = mean_squared_error(y_test_classes, y_test_pred_classes)

# Print metrics
print(f"Training MAE: {train_mae:.4f}, Validation MAE: {val_mae:.4f}, Test MAE: {test_mae:.4f}")
print(f"Training MSE: {train_mse:.4f}, Validation MSE: {val_mse:.4f}, Test MSE: {test_mse:.4f}")

# Calculate accuracy
train_accuracy = np.mean(y_train_classes == y_train_pred_classes)
val_accuracy = np.mean(y_val_classes == y_val_pred_classes)
test_accuracy = np.mean(y_test_classes == y_test_pred_classes)

print(f"Training Accuracy: {train_accuracy:.4f}")
print(f"Validation Accuracy: {val_accuracy:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")


# Calculate Weighted Kappa for training, validation, and test sets
train_kappa = cohen_kappa_score(y_train_classes, y_train_pred_classes, weights='quadratic')
val_kappa = cohen_kappa_score(y_val_classes, y_val_pred_classes, weights='quadratic')
test_kappa = cohen_kappa_score(y_test_classes, y_test_pred_classes, weights='quadratic')

# Print the Kappa scores
print(f"Training Weighted Kappa: {train_kappa:.4f}")
print(f"Validation Weighted Kappa: {val_kappa:.4f}")
print(f"Test Weighted Kappa: {test_kappa:.4f}")

# =========================================================
# 9. Bootstrapped 95% CI (Test set, S only)
# =========================================================

n_bootstraps = 1000
rng = np.random.RandomState(42)
indices = np.arange(len(y_test_classes))

boot_acc   = []
boot_mae   = []
boot_kappa = []

for i in range(n_bootstraps):
    s = rng.choice(indices, size=len(indices), replace=True)
    y_true_bs = y_test_classes[s]
    y_pred_bs = y_test_pred_classes[s]

    boot_acc.append(np.mean(y_true_bs == y_pred_bs))
    boot_mae.append(mean_absolute_error(y_true_bs, y_pred_bs))
    boot_kappa.append(cohen_kappa_score(y_true_bs, y_pred_bs, weights='quadratic'))

def ci(v):
    return np.percentile(v, 2.5), np.percentile(v, 97.5)

acc_ci   = ci(boot_acc)
mae_ci   = ci(boot_mae)
kappa_ci = ci(boot_kappa)

print("\nBootstrapped 95% Confidence Intervals (S-only Test set):")
print(f"Accuracy: {test_accuracy:.4f} (95% CI: {acc_ci[0]:.4f} – {acc_ci[1]:.4f})")
print(f"MAE:      {test_mae:.4f} (95% CI: {mae_ci[0]:.4f} – {mae_ci[1]:.4f})")
print(f"Kappa:    {test_kappa:.4f} (95% CI: {kappa_ci[0]:.4f} – {kappa_ci[1]:.4f})")

# Define labels for the categories
labels = np.arange(1, 5)  # [1, 2, 3, 4]
class_labels = ['S1', 'S2', 'S3', 'S4']

# Ensure all categories are represented in the confusion matrix
conf_matrix = confusion_matrix(y_test_classes, y_test_pred_classes, labels=labels)

# Create a distance matrix for the ordinal categories
num_classes = len(labels)
distance_matrix = np.abs(np.arange(num_classes).reshape(-1, 1) - np.arange(num_classes))

# Calculate the weighted confusion matrix
weighted_conf_matrix = conf_matrix * (1 - distance_matrix / (num_classes - 1))

# Normalise the confusion matrix for better interpretability
normalised_conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1, keepdims=True)
weighted_normalised_conf_matrix = weighted_conf_matrix / weighted_conf_matrix.sum(axis=1, keepdims=True)

# Plot the weighted normalised confusion matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    weighted_normalised_conf_matrix,
    annot=True,
    fmt=".2f",
    cmap="Blues",
    xticklabels=class_labels,
    yticklabels=class_labels
)
plt.title("Weighted Normalised Confusion Matrix for Ordinal Categories")
plt.xlabel("Predicted Category")
plt.ylabel("True Category")
plt.show()

# Plot training & validation MAE and loss
if 'history' in locals() and history.history:
    plt.figure(figsize=(14, 5))

    # MAE
    plt.subplot(1, 2, 1)
    plt.plot(history.history.get('mae', []), label='Training MAE')
    plt.plot(history.history.get('val_mae', []), label='Validation MAE')
    plt.title('Model MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history.get('loss', []), label='Training Loss')
    plt.plot(history.history.get('val_loss', []), label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()